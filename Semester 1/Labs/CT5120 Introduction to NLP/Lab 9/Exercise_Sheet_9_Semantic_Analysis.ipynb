{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79MQN4eMlBCq"
   },
   "source": [
    "# Learning Objectives\n",
    "In this lab we are going to:\n",
    "- Explore using semantic features to improve intent classification\n",
    "- Explore word sense disambiguation (WSD) with Wordnet/NLTK\n",
    "- WSD using the lesk algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zs1UOAvkKl6"
   },
   "source": [
    "# Setup \n",
    "Please run all the cell in this section before starting exercises. This section install the necessary packages and downloads the data needed for all exercises below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jP-UE7vYPkJG",
    "outputId": "8a65c61f-f327-4673-fad9-93ea13fdb3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished installing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets >> dev.null\n",
    "!pip install swifter >> dev.null\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"finished installing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zMTdojpkgjc",
    "outputId": "242fe183-9910-43d8-9a9a-05a6a09b45db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset split sizes: train - 7500 | test - 4500\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Grab Clinc data and convert them into data frames\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_small.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "train  = pd.DataFrame(data[\"train\"], columns=[\"text\", \"intent\"])\n",
    "train[\"split\"] = \"train\"\n",
    "\n",
    "test   = pd.DataFrame(data[\"test\"], columns=[\"text\", \"intent\"])\n",
    "test[\"split\"] = \"test\"\n",
    "\n",
    "print(f\"dataset split sizes: train - {len(train)} | test - {len(test)}\")\n",
    "\n",
    "# Combine datasets into single dataframe for easier use\n",
    "dataset = pd.concat([train, test])\n",
    "\n",
    "# Filter intents \n",
    "labels = [\"application_status\", \"alarm\", \"apr\", \"are_you_a_bot\", \n",
    "          \"balance\", \"calendar_update\", \"calories\", \"carry_on\", \"change_accent\",\n",
    "          \"change_ai_name\", \"change_language\", \"change_volume\", \"exchange_rate\",\n",
    "          \"expiration_date\", \"find_phone\", \"freeze_account\", \"greeting\",\n",
    "          \"insurance_change\", \"jump_start\",\n",
    "          \"interest_rate\",\n",
    "          \"smart_home\",\n",
    "          \"schedule_meeting\",\n",
    "          \"user_name\",\n",
    "          \"w2\",\n",
    "          \"weather\",\n",
    "          \"what_is_your_name\",\n",
    "          \"what_song\",\n",
    "          \"who_do_you_work_for\",\n",
    "          \"yes\"\n",
    "          ]\n",
    "dataset = dataset.query(\"intent in @labels\")\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgWrbXAylXH4"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this lab we'll be learning more about semantic features through the applied task of intent classification. Intent classification is a common feature of most modern conversational AI and chatbot systems. Intent classification aims to understand what a user is telling the bot and map the user's utterance to a prespecified set of intent categories. Let's imagine we're buidling a chatbot for a bank to handle common banking actions like: opening a savings account (open-savings) or reset pin number (reset-pin). We can can create a set of intents like open-savings and reset-pin to represent those actions to the bot. Then when a user says \"I want to open a savings account\" or \"How can I sign up for a savings account\", our classifier model will predict \"open-savings\" intent and respond to the user accordingly. \n",
    "\n",
    "We can use machine learning to build a classifier that can automatically map user utterances to intents. For this lab we'll explore creating a simple model for intent classification and using semantic features (e.g. synsets from wordnet and word sense disambiguation) to improve the perfomance our baseline model. \n",
    "\n",
    "## Data\n",
    "We'll be using the Clinc-150 dataset which consists of 150 common chatbot intents and training utterances. More information about this dataset can be found in their paper [An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction](https://aclanthology.org/D19-1131/). \n",
    "\n",
    "For simplicity we gone ahead downloaded the data for you and stored it in a pandas dataframe. The variable `dataset` is pandas dataframe consisting of three columns:\n",
    "- text: the user utterance \n",
    "- intent: intent label \n",
    "- split: identifies if the row is from the train or test split\n",
    "\n",
    "\n",
    "We'll use a smaller subset of 150 intents so that we can experiment more quickly. As the goal of the lab is on semantic analysis, we've provided the baseline model for you. However you are encouraged to explore the full dataset and other models once you've finished the exercises below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "HaAXI35kwZoH",
    "outputId": "678e77da-df20-4f63-9a5c-9d403abc6b36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>what am i allowed to carry on for american air...</td>\n",
       "      <td>carry_on</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>what are the carry on rules for united</td>\n",
       "      <td>carry_on</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>what can't i carry-on to delta</td>\n",
       "      <td>carry_on</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tell me united's carry on policy</td>\n",
       "      <td>carry_on</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>what is the carry on limit</td>\n",
       "      <td>carry_on</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>what do i need to do now that my battery is dead</td>\n",
       "      <td>jump_start</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>i have to jump start my car</td>\n",
       "      <td>jump_start</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>what shall i do now that my battery is dead</td>\n",
       "      <td>jump_start</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>i gotta jump start my car</td>\n",
       "      <td>jump_start</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>what is the process for jump starting my car</td>\n",
       "      <td>jump_start</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2320 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      intent  split\n",
       "50    what am i allowed to carry on for american air...    carry_on  train\n",
       "51               what are the carry on rules for united    carry_on  train\n",
       "52                       what can't i carry-on to delta    carry_on  train\n",
       "53                     tell me united's carry on policy    carry_on  train\n",
       "54                           what is the carry on limit    carry_on  train\n",
       "...                                                 ...         ...    ...\n",
       "4285   what do i need to do now that my battery is dead  jump_start   test\n",
       "4286                        i have to jump start my car  jump_start   test\n",
       "4287        what shall i do now that my battery is dead  jump_start   test\n",
       "4288                          i gotta jump start my car  jump_start   test\n",
       "4289       what is the process for jump starting my car  jump_start   test\n",
       "\n",
       "[2320 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of dataset variable\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NglQul4ym7zT"
   },
   "source": [
    "## Baseline Model Preprocessing and setup\n",
    "\n",
    "Recall from previous lectures that building a machine learning NLP model consists of several steps. \n",
    "\n",
    "- First we encode our categorical labels into numerical values that our classifier will be predicting. To accomplish this we use sklearns LabelEncoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). We generate two variables `y_train` and `y_test` which contain our encoded labels in to list.\n",
    "\n",
    "- Next we create input features for our model. As the input to our model is user utterances (text), we'll represent each sentence as a vector of `tf-idf` scores across a learned vocabulary. We use the `TfidfVectorizer` (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to fit and transform our user utterances into tfidf features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoWDefT0lnLK",
    "outputId": "9c29125d-d8db-4fa7-8176-7a3b2d586935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Encode intent label and transform into enumerated values\n",
    "le = LabelEncoder()\n",
    "le.fit(dataset[\"intent\"])\n",
    "\n",
    "dataset[\"encoded_label\"] = le.transform(dataset[\"intent\"])\n",
    "\n",
    "# break out the encoded labels by train / test split\n",
    "y_train = dataset.query(\"split=='train'\")[\"encoded_label\"]  \n",
    "y_test  = dataset.query(\"split=='test'\")[\"encoded_label\"]\n",
    "\n",
    "# Generate a vocabaulary from text and generate tfidf features \n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(dataset[\"text\"])\n",
    "\n",
    "# create train and text input features\n",
    "X_train = tfidf.transform(dataset.query(\"split=='train'\")[\"text\"])\n",
    "X_test =  tfidf.transform(dataset.query(\"split=='test'\")[\"text\"])\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyU2kCJWy0lA"
   },
   "source": [
    "## Baseline Model Implementation\n",
    "\n",
    "Finally we go ahead and train our model. We'll use Gaussian Naive Bayes(https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) with the default hyper parameters. Feel free to read the documention to experiment with the hyperparameters and see how it effects the model.\n",
    "\n",
    "\n",
    "Finally we'll evaluate the model on our test set by first generating predictions used the trained model and then evaluating them against our gold labels `y_test`. We use sklearn's `accuracy_score` to get the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lihpfssXoilR",
    "outputId": "e39eecf2-6de9-4d78-e113-11714a0ff884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - 0.7988505747126436\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load model and fit to training data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "# Generate predictions\n",
    "preds = clf.predict(X_test.toarray())\n",
    "\n",
    "# Calculate Accuracy metrics\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy - {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ykMx_7kswTA"
   },
   "source": [
    "# Semantic features with Wordnet\n",
    "\n",
    "Let's get to main exerises for this week. Recall that semantics involves understanding the meaning of words and concepts. Semantic analysis of text explores specific meaning of words in context (word sense), words that related together through common definitions (synoynms) or perhaps diameterically different from each other (antonyms) and in many other ways. In addition to the slides, as useful introduction can be found here: [Overview and Semantic Issues of Text Mining](https://sigmodrecord.org/publications/sigmodRecord/0709/p23.cesar-andritsos.pdf)\n",
    "\n",
    "## Wordnet Synsets\n",
    "We'll be using WordNet as the source of our semantic features and tools. \n",
    "\n",
    "\n",
    "\"WordNet is an on-line lexical reference system whose design is inspired by current psycholinguistic theories of human lexical memory. English nouns, verbs, and adjectives are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets.\" [1]\n",
    "\n",
    "The NLTK library has a useful set of APIs to access Wordnet and perform common semantic analysis tasks. More information can be found in the documentation here: https://www.nltk.org/howto/wordnet.html.\n",
    "\n",
    "In this exercise we'll be generating synoynms from wordnet synsets. The goal is augment the input to the model with additional synonyms for the key nouns to give the model additional semantic clues as to the meaning of the input. \n",
    "\n",
    "Synsets are an NLTK object which are a group of synonymous words that are related to a specific concept. \n",
    "\n",
    "The synset object has several useful properties and methods. Each synset contains a dictionary definition, related synonyms, and many other properties that can be found in the documentation.  \n",
    "\n",
    "Let's a take a closer look in the code below:\n",
    "\n",
    "[1] Introduction to WordNet: An On-line Lexical Database (https://wordnetcode.princeton.edu/5papers.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jZEg4OZHwXKM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('balance.n.01')\n",
      "Defintion:  a state of equilibrium\n",
      "Synonym lemmas:  ['balance']\n",
      "-----------------------------------\n",
      "Synset('balance.n.02')\n",
      "Defintion:  equality between the totals of the credit and debit sides of an account\n",
      "Synonym lemmas:  ['balance']\n",
      "-----------------------------------\n",
      "Synset('proportion.n.05')\n",
      "Defintion:  harmonious arrangement or relation of parts or elements within a whole (as in a design); - John Ruskin\n",
      "Synonym lemmas:  ['proportion', 'proportionality', 'balance']\n",
      "-----------------------------------\n",
      "Synset('balance.n.04')\n",
      "Defintion:  equality of distribution\n",
      "Synonym lemmas:  ['balance', 'equilibrium', 'equipoise', 'counterbalance']\n",
      "-----------------------------------\n",
      "Synset('remainder.n.01')\n",
      "Defintion:  something left after other parts have been taken away\n",
      "Synonym lemmas:  ['remainder', 'balance', 'residual', 'residue', 'residuum', 'rest']\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Let's take a looking at the synset for the word balance in Wordnet\n",
    "\n",
    "# Synsets is a collection of synset obects. Here the first 5 synsets related to \n",
    "# the word balance\n",
    "for synset in wn.synsets(\"balance\")[:5]:\n",
    "  print(synset)\n",
    "  print(\"Defintion: \",synset.definition())\n",
    "  print(\"Synonym lemmas: \", synset.lemma_names() )\n",
    "  print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SCq13iw7lL4"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "The overall goal of this exercise is to agument each input sentence with a set of related synonyms for the nouns found in the sentence. At a high level we can accomplish this with the following steps:\n",
    "\n",
    "1. Extract all nouns in the sentence\n",
    "2. For each noun look up synonyms (if they exist) from the Wordnet synsets\n",
    "3. Add the extracted synonyms to the end of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOLorYsu6Qom"
   },
   "source": [
    "## Exercise 1a. Extract all nouns in a sentence\n",
    "For this exercise you will write a function that extracts all the nouns in a sentence. \n",
    "\n",
    "For example:\n",
    "\n",
    "- Sentence: I want to check the balance of my bank account\n",
    "- Nouns: balance, bank, account \n",
    "\n",
    "To identify nouns we can use a NLTK POS tagger to tag each word in the sentence with POS tag. We can then extract all word with the relevant POS tags that define nouns. Below we show how you can get POS tags from NLTK for a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "eodOlApnRBMm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRP\n",
      "want VBP\n",
      "to TO\n",
      "check VB\n",
      "the DT\n",
      "balance NN\n",
      "of IN\n",
      "my PRP$\n",
      "bank NN\n",
      "accounts NNS\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "ex1 = \"I want to check the balance of my bank accounts\"\n",
    "\n",
    "# 1. Tokenize text\n",
    "toks = word_tokenize(ex1)\n",
    "\n",
    "# 2. Generate tagged tokens\n",
    "tagged = nltk.pos_tag(toks)\n",
    "\n",
    "# 3. Loop over tagged tokens and print out word and tag\n",
    "for word, pos_tag in tagged:\n",
    "  print(word, pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUfTFXuDG8hh"
   },
   "source": [
    "Go ahead and write the code to extract the nouns based on POS tags. For a full list of POS tags see: https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/ \n",
    "\n",
    "\n",
    "Hint: the POS tag for singular nouns is NN. Can you find the remaining tags for plural nouns, proper nouns singular and proper noun plural. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EATlO4-NVk-n",
    "outputId": "edae3437-f573-4939-d011-9f194c360b64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance', 'bank', 'accounts']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_nouns(text):\n",
    "  \"\"\"\n",
    "  This method takes in a sentene and return back of extracted nouns. If no nouns\n",
    "  are found, an empty list is returned. \n",
    "  \"\"\"\n",
    "  # 1. Tokenize text\n",
    "  toks = nltk.word_tokenize(text)\n",
    " \n",
    "  # 2. Loop over tagged tokens and extract nouns. Add extracted nouns to the\n",
    "  # extracted nouns list\n",
    "  noun_tags = ['NN', 'NN', 'NNP', 'NNS', 'NNPS']\n",
    "  extracted_nouns = []\n",
    "  \n",
    "  # YOUR CODE BELOW\n",
    "  tagged = nltk.pos_tag(toks)\n",
    "  for word, tag in tagged:\n",
    "        if tag in noun_tags:\n",
    "            extracted_nouns.append(word)\n",
    " \n",
    "  return extracted_nouns\n",
    "\n",
    "extract_nouns(\"I want to check the balance of my bank accounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxdZJFgpS6yO"
   },
   "source": [
    "## Exercise 1b. Extract synonyms from Wordnet \n",
    "Next you will create function to extract all the synonymns for a provided word from Wordnet. In the function below write your code to get synonyms from from Wordnet.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7tdPc6hW51a",
    "outputId": "d317cda9-8c1c-4a5f-b0e8-4aa312f9001b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance',\n",
       " 'balance',\n",
       " 'proportion',\n",
       " 'proportionality',\n",
       " 'balance',\n",
       " 'balance',\n",
       " 'equilibrium',\n",
       " 'equipoise',\n",
       " 'counterbalance',\n",
       " 'remainder',\n",
       " 'balance',\n",
       " 'residual',\n",
       " 'residue',\n",
       " 'residuum',\n",
       " 'rest',\n",
       " 'balance',\n",
       " 'Libra',\n",
       " 'Balance',\n",
       " 'Libra',\n",
       " 'Libra_the_Balance',\n",
       " 'Balance',\n",
       " 'Libra_the_Scales',\n",
       " 'symmetry',\n",
       " 'symmetricalness',\n",
       " 'correspondence',\n",
       " 'balance',\n",
       " 'counterweight',\n",
       " 'counterbalance',\n",
       " 'counterpoise',\n",
       " 'balance',\n",
       " 'equalizer',\n",
       " 'equaliser',\n",
       " 'balance_wheel',\n",
       " 'balance',\n",
       " 'balance',\n",
       " 'balance',\n",
       " 'equilibrate',\n",
       " 'equilibrize',\n",
       " 'equilibrise',\n",
       " 'balance',\n",
       " 'poise',\n",
       " 'balance',\n",
       " 'balance']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_synonyms(noun):\n",
    "  \"\"\"\n",
    "  Given a noun, return all the synonyms (if any) found in the synsets for\n",
    "  that noun. Return an empty list if none are found. \n",
    "  \"\"\"\n",
    "  synonyms = []\n",
    "\n",
    "  # Loop over the synsets for a given word and add each synonym \n",
    "  # to the synonyms list. \n",
    "  for synset in wn.synsets(noun):\n",
    "        synonyms.extend(synset.lemma_names())\n",
    "\n",
    "  return(synonyms)\n",
    "\n",
    "extract_synonyms(\"balance\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amfFZ4Spbs2W"
   },
   "source": [
    "## Exercise 1c. Add extracted synonyms to end of input\n",
    "\n",
    "Finally, you will create a function that combines the functions you wote in 1a and 1b. In this function, you'll take all the synonyms found and add them to the end of the input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e2xG8j7EkDVF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: I want to check the balance of my bank accounts\n",
      "new text: I want to check the balance of my bank accounts counterbalance swear camber equilibrium equaliser rest balance_wheel chronicle residue rely correspondence report counterweight banking_company depository_financial_institution poise coin_bank proportion explanation trust equilibrize Libra_the_Balance story accounting calculate symmetricalness Libra_the_Scales banking_concern residuum score equilibrise symmetry money_box news_report remainder savings_bank equipoise history account counterpoise deposit equalizer cant Balance write_up account_statement residual bank business_relationship invoice Libra proportionality bank_building describe answer_for bill equilibrate balance\n"
     ]
    }
   ],
   "source": [
    "def add_synonyms_to_text(text):\n",
    "  all_synonyms = []\n",
    " \n",
    "  # 1. Extract Nouns\n",
    "  extracted_nouns = extract_nouns(text)# your code here\n",
    "\n",
    "  # Edge case, no nouns are found\n",
    "  if extracted_nouns == []:\n",
    "    return text\n",
    "\n",
    "  # 2. For each noun extract synonyms\n",
    "  for noun in extracted_nouns:\n",
    "    # 1. Get synonyms\n",
    "    s = extract_synonyms(noun)# your code here\n",
    "    \n",
    "    # 2. Add it to our list of synonyms and grab a unique\n",
    "    all_synonyms.extend(s)\n",
    "    unique_synonyms = set(all_synonyms)\n",
    "    \n",
    "  # 3. Return input with extracted synonyms appended at the end\n",
    "  new_text = text + \" \" + \" \".join(unique_synonyms)\n",
    "  if new_text is None:\n",
    "    return text\n",
    "  else:\n",
    "    return new_text\n",
    "\n",
    "print(f\"original text: {ex1}\")\n",
    "print(f\"new text: {add_synonyms_to_text(ex1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EO6hZaYcuiA"
   },
   "source": [
    "## Evaluation of Synonym features\n",
    "\n",
    "Now we're ready to see what the effect of these semantic feature are on our model. We will want to apply the `add_synonyms_to_text` method to all the \n",
    "text in our dataset. Doing this linearly in a for loop is very inefficient. So provide code that parallelizes the operation below. We also provide the code to run to retrain the model. After running the cells below go to Exercise 1d to wrap up Exercise 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2lgPnThjalAG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages/swifter/swifter.py:87: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de83da4269544577ad1cc204e936d430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "# Apply the text with synonyms function to text column in datasets\n",
    "dataset[\"text_with_synonyms\"] = dataset[\"text\"].swifter.apply(lambda x: add_synonyms_to_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bzMjPHZWxul0"
   },
   "outputs": [],
   "source": [
    "# Generate inputs \n",
    "tfidf_w_synonyms = TfidfVectorizer()\n",
    "tfidf_w_synonyms.fit(dataset[\"text_with_synonyms\"])\n",
    "\n",
    "X_train = tfidf_w_synonyms.transform(dataset.query(\"split=='train'\")[\"text_with_synonyms\"])\n",
    "X_test =  tfidf_w_synonyms.transform(dataset.query(\"split=='test'\")[\"text_with_synonyms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3J6_cCCOJlrN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.803448275862069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "preds = clf.predict(X_test.toarray())\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y14bzHQfJbq"
   },
   "source": [
    "## Exercise 1d: Evaluation\n",
    "Did adding synonyms to the input improve performance? What is a potential downside of this approach?\n",
    "\n",
    "Write your answer here:\n",
    "\n",
    "it did, but we may be adding words with different meaning and changed the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP2E-BeENNW3"
   },
   "source": [
    "# Improving Synonym Selection with WSD and Lesk\n",
    "\n",
    "One limitation of the approach above was that we were naively adding all the synonyms we found in a synset for a given word. However, words can have different meanings in different contexts. Recall the concept of word sense and the problem of word sense disambiguation. Word sense disambiguation aims to identify the specific meaning of a word in context. \n",
    "\n",
    "The lesk algorithm aims to disambugiuate a word based on the overlap between the context around the word and the definitions of the word. The definition that has the highest similarity to the conctext is returned. \n",
    "\n",
    "NLTK has an implementation of the lesk algorithm which we explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k1Fil0hYQtYx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He cashed a check at the bank.\n",
      "Synset('savings_bank.n.02')\n",
      "Definition:  a container (usually with a slot in the top) for keeping money at home\n",
      "Synonyms:  ['savings_bank', 'coin_bank', 'money_box', 'bank']\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "He saw the road bank left.\n",
      "Synset('bank.n.07')\n",
      "Definition:  a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synonyms:  ['bank', 'cant', 'camber']\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "sent = 'He cashed a check at the bank.'\n",
    "sent_toks = nltk.word_tokenize(sent)\n",
    "ambiguous = 'bank'\n",
    "\n",
    "syn = lesk(sent_toks, ambiguous, pos='n')\n",
    "print(sent)\n",
    "print(syn)\n",
    "print(\"Definition: \", syn.definition())\n",
    "print(\"Synonyms: \", syn.lemma_names())\n",
    "\n",
    "print(\"\\n------------------------------------------\\n\")\n",
    "sent2 = 'He saw the road bank left.'\n",
    "sent2_toks = nltk.word_tokenize(sent2)\n",
    "ambiguous = 'bank'\n",
    "syn2 = lesk(sent2_toks, \"bank\")\n",
    "\n",
    "print(sent2)\n",
    "print(syn2)\n",
    "print(\"Definition: \", syn2.definition())\n",
    "print(\"Synonyms: \", syn2.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WP1GS1GFpof2"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Let's create a function `lesk_synonyms` that provides synonyms based on the disambiguated noun. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyfcM7vYW6Yt",
    "outputId": "ddc2c795-1174-4ac1-f108-bb54253c3a9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bank', 'cant', 'camber']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def lesk_synonyms(text, ambiguous):\n",
    "  \"\"\"\n",
    "  Given a context and ambigous noun, return the synonyms from the disambiguated\n",
    "  synset. Return empty list if none are found.\n",
    "  \"\"\"\n",
    "  # 1. Get disambiguated synset\n",
    "  syn = lesk(nltk.word_tokenize(text), ambiguous, pos='n') # your code here\n",
    "\n",
    "  # Edge case, if synset is empty return empty list\n",
    "  if syn is None:\n",
    "    return []\n",
    "\n",
    "  # Strip out '_' found in lemmas that contain compound words.\n",
    "  cleaned_synonyms = [lemma.replace(\"_\", \" \") for lemma in syn.lemma_names()] \n",
    "  return cleaned_synonyms\n",
    "\n",
    "lesk_synonyms(\"I went to the bank today.\", \"bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yknQEHOhqqci"
   },
   "source": [
    "We updated the `add_synonyms_to_text` to use the lesk synonyms and updated the model below. After running the cells below, go Exercise 2a. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xRW8P1hWZ8ZY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He went to the bank\n",
      "He went to the bank bank cant camber\n"
     ]
    }
   ],
   "source": [
    "def add_synonyms_to_text(text): \n",
    "  \n",
    "  # 1. Extract Nouns\n",
    "  extracted_nouns = extract_nouns(text)\n",
    "\n",
    "  # Edge case, no nouns are found\n",
    "  if extracted_nouns == []:\n",
    "    return text\n",
    "\n",
    "  all_synonyms = []\n",
    "  for noun in extracted_nouns:\n",
    "    # 1. Get synonyms\n",
    "    s = lesk_synonyms(text, noun)\n",
    "    \n",
    "    # 2. Add it to our list of synoyms\n",
    "    all_synonyms.extend(s)\n",
    "    unique_synonyms = set(all_synonyms)\n",
    "  \n",
    "  if len(unique_synonyms) == 0:\n",
    "    return text\n",
    "  else:\n",
    "   return text + \" \" + \" \".join(unique_synonyms)\n",
    "\n",
    "print(\"He went to the bank\")\n",
    "print(add_synonyms_to_text(\"He went to the bank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "LvmW9Handm-V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages/swifter/swifter.py:87: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee73d1dde2a47759a794ec5c59bb66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"text_with_lesk_synonyms\"] = dataset[\"text\"].swifter.apply(lambda x: add_synonyms_to_text(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9PCSBRrpe-0B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "# Generate inputs \n",
    "tfidf_w_synonyms = TfidfVectorizer()\n",
    "tfidf_w_synonyms.fit(dataset[\"text_with_lesk_synonyms\"])\n",
    "\n",
    "X_train = tfidf_w_synonyms.transform(dataset.query(\"split=='train'\")[\"text_with_lesk_synonyms\"])\n",
    "X_test =  tfidf_w_synonyms.transform(dataset.query(\"split=='test'\")[\"text_with_lesk_synonyms\"])\n",
    "\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VKWWTG-Eg2Wn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train.toarray(), y_train)\n",
    "\n",
    "preds = clf.predict(X_test.toarray())\n",
    "print(accuracy_score(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1LJsdpXrs0K"
   },
   "source": [
    "## Exercise 2a. Evaluation\n",
    "What effect did replacing synonyms with lesk synonyms have on the model's accuracy?\n",
    "\n",
    "it improved the model accuracy, as the synonyms added are disambigouous with lesk hence increased the chances of adding relevant synonyms only."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
