{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import nltk\n","from nltk.corpus import brown\n","from nltk.corpus import treebank\n","from nltk.corpus import conll2000\n","from nltk.tokenize.treebank import TreebankWordDetokenizer\n","from sklearn.model_selection import train_test_split\n","nltk.download('treebank')\n","nltk.download('brown')\n","nltk.download('conll2000')\n","nltk.download('universal_tagset')\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","tokenizer = get_tokenizer('basic_english')"],"metadata":{"id":"zFLnTqsGdOjO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666321691990,"user_tz":-60,"elapsed":1786,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"7530f2e9-6e6b-4cf1-81cf-9a9dc3cdf019"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]   Package conll2000 is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["# PART OF SPEECH TAGGING\n","\n","### Data Preparation\n","\n","1. Concatenate all datasets\n","\n","\n","\n","\n"],"metadata":{"id":"EPF0hI8AdNmE"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"NgNz3k5GcqBs","executionInfo":{"status":"ok","timestamp":1666321708150,"user_tz":-60,"elapsed":16161,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"outputs":[],"source":["## Combining all the datasets to form one big dataset \n","treebank_corpus = treebank.tagged_sents(tagset='universal')\n","brown_corpus = brown.tagged_sents(tagset='universal')\n","conll_corpus = conll2000.tagged_sents(tagset='universal')\n","\n","## Concatenate the dataset to form a big corpus of type (word, label)\n","tag_dataset_corpus = treebank_corpus + brown_corpus + conll_corpus\n","\n","## making dataset smaller for faster training \n","tag_dataset_corpus = tag_dataset_corpus[:7500]\n"]},{"cell_type":"markdown","source":["2. Break the dataset into Data, Label arrays i.e:\n","\n","| Sentence | Sequence label |\n","| -------- | -------------- |\n","|this happened that day| POS_1 POS_2... POS_N |\n","|this happened the other day| POS_1 POS_2... POS_N |\n","\n","3. We need all our training data to exist in numerical form for it to be trained by Neural Networks.\n","Towards this end we encode all our labels to numeric values."],"metadata":{"id":"Uv7L6XMU6jPO"}},{"cell_type":"code","source":["X, yx = [], []\n","result_set = set()\n","tbd = TreebankWordDetokenizer()\n","\n","## getting all sentences in X and all labels in yx from the corpus\n","for instance in tag_dataset_corpus:\n","    x = list(map(lambda x: x[0], instance))\n","    y = list(map(lambda x: x[1], instance))\n","    X.append(tbd.detokenize(x))\n","    yx.append(y)\n","    for tag in y:\n","        result_set.add(tag)\n","\n","# making the tag dictionary that will encode all the POS tags to numeric values\n","tag_encoding = {t:i+1 for i, t in enumerate(result_set)}\n","tag_encoding[''] = 0\n"],"metadata":{"id":"Y7y8vqIUeTd1","executionInfo":{"status":"ok","timestamp":1666321711174,"user_tz":-60,"elapsed":3025,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 1\n","Use the dictionary `tag_encoding` to encode all the POS tags to numerical value."],"metadata":{"id":"55XObPxH8u2c"}},{"cell_type":"code","source":["## all the POS tags bu\n","y_encoded = []\n","\n","for y in yx:\n","    y_i_encoded = [tag_encoding[tag] for tag in y]\n","    y_encoded.append(y_i_encoded)"],"metadata":{"id":"Wopn-M4K-UjL","executionInfo":{"status":"ok","timestamp":1666321711175,"user_tz":-60,"elapsed":8,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 2\n","Split the dataset into training and testing dataset. Use the encoded labels obtained in previous exercise instead of the string labels for POS tagging.\n","\n","Split in a way that test size is 20% of the dataset."],"metadata":{"id":"YDu351rh-qVZ"}},{"cell_type":"code","source":["train_X, test_X, train_y, test_y = train_test_split(X, y_encoded, test_size=0.20)\n"],"metadata":{"id":"FW5q0j8reVxO","executionInfo":{"status":"ok","timestamp":1666321711175,"user_tz":-60,"elapsed":7,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Just as we encoded the labels to have a numerical value we need to encode the words in the text too.\n","\n","This possesses many challenges:\n","1. We want to limit the vocabulary based on the frequency of occurance.\n","2. The size of the vocabulary shouldn't be extraordinarily large as it would just make lookup operation very expensive.\n","3. All out of vocabulary words have to be recognised and labelled as such.\n","\n","Therefore we use the suitable implemented funtion."],"metadata":{"id":"V1-oDC5J_fgz"}},{"cell_type":"code","source":["def yield_tokens(data_iter):\n","    for text in data_iter:\n","        yield tokenizer(text)\n","\n","vocab = build_vocab_from_iterator(yield_tokens(train_X), specials=['<unk>'], max_tokens=20_000)\n","vocab.set_default_index(vocab[\"<unk>\"])"],"metadata":{"id":"Yw-Wwikpvo32","executionInfo":{"status":"ok","timestamp":1666321711175,"user_tz":-60,"elapsed":7,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Creating Custom dataset\n","We need this class since we would be feeding lots of training data in batches so certain transformations and padding/truncation is required to make the training process fast and robust.\n","\n","We truncate/pad to the size of each input being 64. All inputs will have a 64 number long vector.\n","\n","[Dataset & DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)"],"metadata":{"id":"bZjLFpbUCZh-"}},{"cell_type":"code","source":["class PosDataset(Dataset):\n","    def __init__(self, X, y, vocab, transform=None, padding=128):\n","        self.X = X\n","        self.y = y\n","        self.vocab = vocab\n","        self.tokenizer = get_tokenizer('basic_english')\n","        self.padding = padding\n","    \n","    @staticmethod\n","    def pad_truncate_tensor(vec, pad):\n","        \"\"\"\n","        args:\n","            vec - tensor to pad\n","            pad - the size to pad to\n","            dim - dimension to pad\n","\n","        return:\n","            a new tensor padded to 'pad' in dimension 'dim'\n","        \"\"\"\n","        # pad_size = list(vec.shape)\n","        # pad_size[dim] = pad - vec.size(dim)\n","        # return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\n","        inp_length = vec.shape[-1]\n","        if inp_length > pad:\n","            return vec[:pad]\n","        else:\n","            pad_length = pad - vec.shape[-1]\n","            return torch.cat([vec, torch.zeros(pad_length, dtype=torch.int64)])\n","    \n","    def __getitem__(self, index):\n","        x = self.X[index]\n","        y = self.y[index]\n","        x_encoded = self.vocab(self.tokenizer(x))\n","        \n","        return self.pad_truncate_tensor(torch.tensor(x_encoded, dtype=torch.int64), 64), self.pad_truncate_tensor(torch.tensor(y, dtype=torch.int64), 64)\n","    \n","    def __len__(self):\n","        return len(self.X)\n","\n","train_dataset = PosDataset(train_X, train_y, vocab)\n","test_dataset  = PosDataset(test_X, test_y, vocab)"],"metadata":{"id":"m8MEl5tq0li9","executionInfo":{"status":"ok","timestamp":1666321711175,"user_tz":-60,"elapsed":6,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["for p in train_dataset:\n","  print(p)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85c0-ROLG5H-","executionInfo":{"status":"ok","timestamp":1666321711175,"user_tz":-60,"elapsed":6,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"68cafb40-aa9d-4caf-b04f-2633d05f4b97"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor([  22,  821,   12, 2359,   46,   27,  112,   11,   54,   66,    1,   10,\n","           1, 1599,    8,   54,   66,    1,   10,    1,  720, 5520,    1,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0]), tensor([11, 10, 12,  7, 10, 10, 10, 12,  6,  7,  7,  4,  6,  7,  7,  7,  2,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n","         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))\n"]}]},{"cell_type":"markdown","source":["## Data Loaders \n","Now we create use the datasets to create dataloaders which we can directly feed to our models in batches.\n"],"metadata":{"id":"JOaeFvuUHWIm"}},{"cell_type":"code","source":["BATCH_SIZE = 64\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"],"metadata":{"id":"LNcd9rwM0t3A","executionInfo":{"status":"ok","timestamp":1666321711176,"user_tz":-60,"elapsed":6,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Model creation\n","\n","Next we create the model for POS TAGGING. It takes a sequence of tokens (words) and generates predictions of POS on each of the token.\n","\n","Layers Used:\n","\n","1. [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","\n","2. [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM)\n","\n","3. [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)"],"metadata":{"id":"WJ0-xlW8IfwG"}},{"cell_type":"code","source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n","        super(LSTMTagger, self).__init__()\n","        self.hidden_dim = hidden_dim\n","\n","        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","\n","        # The LSTM takes word embeddings as inputs, and outputs hidden states\n","        # with dimensionality hidden_dim.\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","\n","        # The linear layer that maps from hidden state space to tag space\n","        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.word_embeddings(sentence)\n","        lstm_out, _ = self.lstm(embeds)\n","        tag_space = self.hidden2tag(lstm_out)\n","        return tag_space\n","\n","model = LSTMTagger(100, 64, 20000, 13)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)"],"metadata":{"id":"1_jLrdaB3Hee","executionInfo":{"status":"ok","timestamp":1666321711176,"user_tz":-60,"elapsed":6,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Training Loop\n","\n","1. Forward Propagation: It is defined by us when we implemented the model. During the forward pass the model generates the outputs from the input data.\n","\n","2. Backpropagation: These are the series of steps which use the outputs generated during the forward propagation, compare the output with the true label to calculate error.\n","And update the parameters(weights) of each layer to minimize this error.\n","\n","[Training](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html)"],"metadata":{"id":"yQmldT-_J6Jh"}},{"cell_type":"code","source":["def train(model, epochs=3):\n","  for epoch in range(epochs):\n","      epoch_loss = 0.0\n","      running_loss = 0.0\n","      total_elem = 0\n","      for i, (X,y) in enumerate(train_dataloader):\n","          optimizer.zero_grad()\n","          tag_scores = model(X)\n","          tag_scores = tag_scores\n","          # print('tag_scores', tag_scores.shape)\n","          tag_scores = tag_scores.permute(0, 2, 1) \n","          # print('tag_scores reshape',tag_scores.shape, y.shape)     \n","          loss = loss_function(tag_scores, y)\n","          loss.backward()\n","          optimizer.step()\n","          epoch_loss += tag_scores.shape[0] * loss.item()\n","          running_loss += loss.item()\n","          total_elem += tag_scores.shape[0]\n","      print(f'Epoch:{epoch+1}, Loss: {epoch_loss/total_elem}')\n","  return model"],"metadata":{"id":"tWeBbwzc9CJM","executionInfo":{"status":"ok","timestamp":1666321711176,"user_tz":-60,"elapsed":6,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["model = train(model, 10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yaUn2x6Y9YSL","executionInfo":{"status":"ok","timestamp":1666321830909,"user_tz":-60,"elapsed":119738,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"83ba55b7-5137-4399-e259-f206410270ac"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch:1, Loss: 0.7093473928769429\n","Epoch:2, Loss: 0.4752861246267954\n","Epoch:3, Loss: 0.39774378752708434\n","Epoch:4, Loss: 0.3376055676937103\n","Epoch:5, Loss: 0.2882592797279358\n","Epoch:6, Loss: 0.25685491716861725\n","Epoch:7, Loss: 0.22053154083093007\n","Epoch:8, Loss: 0.1865672868490219\n","Epoch:9, Loss: 0.1621235715150833\n","Epoch:10, Loss: 0.13715872544050217\n"]}]},{"cell_type":"markdown","source":["# Exercise 3\n","Use the `tag_encoding` dictionary created at the start to create `tag_decoder_dict` which maps tag indices to tag names"],"metadata":{"id":"7mvSwXd-Ps4S"}},{"cell_type":"code","source":["tag_decoder_dict = {v:k for k,v in tag_encoding.items()}"],"metadata":{"id":"iC5hfpoLSDPk","executionInfo":{"status":"ok","timestamp":1666321830909,"user_tz":-60,"elapsed":5,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# Exercise 4\n","Implement a function to use the trained model in order to generate POS Tag indices.\n","\n","In that function use the `tag_decoder_dict` from the previous exercise to convert the resulting indices from the model to proper POS tags."],"metadata":{"id":"Y_g7VCpqOuWm"}},{"cell_type":"code","source":["def get_pos(sentence):\n","  k = vocab(tokenizer(sentence))\n","  q = model(torch.tensor(k))\n","  p = q.argmax(dim=1)\n","  return [tag_decoder_dict[ind] for ind in p.tolist()]\n","\n","print(get_pos('It is a big house'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"on2o7l6JSdz0","executionInfo":{"status":"ok","timestamp":1666321830909,"user_tz":-60,"elapsed":4,"user":{"displayName":"gaurav negi","userId":"17788159321082264085"}},"outputId":"cd92b74d-04ab-4557-ab20-2db6a1f52358"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['PRON', 'VERB', 'DET', 'ADJ', 'NOUN']\n"]}]},{"cell_type":"markdown","source":["# Evalutation of test data"],"metadata":{"id":"MOAWBvKNSvsP"}}]}