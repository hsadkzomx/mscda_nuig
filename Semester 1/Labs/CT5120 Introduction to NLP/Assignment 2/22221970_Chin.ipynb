{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G6saNLUFqHb"
   },
   "source": [
    "# Assignment 2 - CT5120\n",
    "\n",
    "### Instructions:\n",
    "- Complete all the tasks below and upload your submission as a Python notebook on Blackboard with the filename “`StudentID_Lastname.ipynb`” before **23:59** on **November 25, 2022**.\n",
    "- This is an individual assignment, you **must not** work with other students to complete this assessment.\n",
    "- The assignment is worth $50$ marks and constitutes 19% of the final grade. The breakdown of the marking scheme for each task is as follows:\n",
    "\n",
    "| Task | Marks for write-up | Marks for code | Total Marks |\n",
    "| :--- | :----------------- | :------------- | :---------- |\n",
    "| 1    |                  5 |              5 |          10 |\n",
    "| 2    |                  - |             10 |          10 |\n",
    "| 3    |                  5 |              5 |          10 |\n",
    "| 4    |                  5 |              5 |          10 |\n",
    "| 5    |                  5 |              5 |          10 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWSEtNeGMsN"
   },
   "source": [
    "---\n",
    "\n",
    "This assignment involves tasks for feature engineering, training and evaluating a classifier for suggestion detection. You will work with the data from SemEval-2019 Task 9 subtask A to classify whether a piece of text contains a suggestion or not. \n",
    "\n",
    "\n",
    "Download train.csv, test_seen.csv and test_unseen.csv from the [Github](https://github.com/sharduls007/Assignment_2_CT5120) or uncomment the code cell below to get the data as a comma-separated values (CSV) file. The CSV file contains a header row followed by 5,440 rows in train.csv and 1,360 rows in test_seen.csv spread across 3 columns of data. Each row of data contains a unique id, a piece of text and a label assigned by an annotator. A label of $1$ indicates that the given text contains a suggestion while a label of $0$ indicates that the text does not contain a suggestion.\n",
    "\n",
    "You can find more details about the dataset in Sections 1, 2, 3 and 4 of [SemEval-2019 Task 9: Suggestion Mining from Online Reviews and Forums\n",
    "](https://aclanthology.org/S19-2151/).\n",
    "\n",
    "We will be using test_seen.csv for benchmarking our model, hence it has label. On the other hand, test_unseen is used for [Kaggle](https://www.kaggle.com/competitions/nlp2022ct5120suggestionmining/overview) competition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShQ2lPxmPfA4",
    "outputId": "df651146-abe3-4d3b-8960-23eb1d2b977b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  670k  100  670k    0     0  1473k      0 --:--:-- --:--:-- --:--:-- 1499k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  168k  100  168k    0     0   581k      0 --:--:-- --:--:-- --:--:--  609k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  208k  100  208k    0     0   658k      0 --:--:-- --:--:-- --:--:--  684k\n"
     ]
    }
   ],
   "source": [
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/train.csv\" > train.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_seen.csv\" > test.csv\n",
    "!curl \"https://raw.githubusercontent.com/sharduls007/Assignment_2_CT5120/master/test_unseen.csv\" > test_unseen.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5x0c38rCGk23"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file.\n",
    "train_df = pd.read_csv('train.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "test_df = pd.read_csv('test.csv', \n",
    "                 names=['id', 'text', 'label'], header=0)\n",
    "\n",
    "# Store the data as a list of tuples where the first item is the text\n",
    "# and the second item is the label.\n",
    "train_texts, train_labels = train_df[\"text\"].to_list(), train_df[\"label\"].to_list() \n",
    "test_texts, test_labels = test_df[\"text\"].to_list(), test_df[\"label\"].to_list() \n",
    "\n",
    "# Check that training set and test set are of the right size.\n",
    "assert len(test_texts) == len(test_labels) == 1360\n",
    "assert len(train_texts) == len(train_labels) == 5440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4106 1334\n",
      "1027 333\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.count(0), train_labels.count(1))\n",
    "print(test_labels.count(0), test_labels.count(1))\n",
    "\n",
    "#imbalanced classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_Scj45oSpdQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 1: Data Pre-processing (10 Marks)\n",
    "\n",
    "Explain at least 3 steps that you will perform to preprocess the texts before training a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Pd8ed8NdlB_"
   },
   "source": [
    "\n",
    "\n",
    "Edit this cell to write your answer below the line in no more than 300 words.\n",
    "\n",
    "---\n",
    "\n",
    "Suggestion detection is a binary classification issue, we are trying to predict the sentence is a suggestion OR not hence the context of the sentence is an important feature. To help the algorithm to understand the context better, we perform the following preprocessing techniques. We strip the quotation marks in the beginning and the end of each sentence. The other punctuations are kept to avoid messing with specific terms. \n",
    "\n",
    "Furthermore, we tokenize the sentences in order to perform the following processing steps. We then remove the stopwords, this step is to remove the possible noise that might negatively affect the performance of the classification and letting the model to focus more on the true meaning of the sentence itself.\n",
    "\n",
    "Other than that, we perform lemmatization to return the base form of the tokens. Lemmatization is chosen instead of stemming as rather than chopping off the ending of words and generating possible nonsense, it is context-dependent hence giving higher accuracy in returning base form of words.  \n",
    "\n",
    "Besides, we perform feature transformation / enrichment by adding synonyms to help the algorithm in disambiguating and emphasizing the context of the sentence. However, instead of getting the nouns, we focus on exploring the synonyms for adjectives and verbs as those might be more related in suggestions. The unique synonyms are retrieved from WordNet. \n",
    "\n",
    "Last but not least, we detokenize the sentences to provide an appropriate input for the following step. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2-xXQggaVKh"
   },
   "source": [
    "In the code cell below, write an implementation of the steps you defined above. You are free to use a library such as `nltk` or `sklearn` for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jb7i3Le4aSYM"
   },
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "import nltk\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords, wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "def data_preprocessing(texts):\n",
    "    processed = []\n",
    "\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    adj_tags = ['JJ', 'JJR', 'JJS']\n",
    "#     noun_tags = ['NN', 'NNP', 'NNS', 'NNPS']\n",
    "#     adverb_tags = [] #['RB', 'RBR', 'RBS']\n",
    "    verb_tags = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for i, sentence in enumerate(texts):\n",
    "        # stripping extra \"\" from sentence + tokenization\n",
    "        tokens = tokenizer.tokenize(sentence[1:-1])\n",
    "        \n",
    "        # to remove stopwords + lemmatize tokens\n",
    "        lemmatized = [lemmatizer.lemmatize(token) for token in tokens if (token not in english_stopwords)]\n",
    "        \n",
    "        # get unique synonyms from wordnet for adjectives and verbs\n",
    "        for word, pos_tag in nltk.pos_tag(lemmatized):\n",
    "            if pos_tag in adj_tags + verb_tags:\n",
    "                # without disambiguation\n",
    "                syns = set(synset.lemma_names()[0].replace(\"_\", \" \") for synset in wn.synsets(word))\n",
    "                lemmatized.extend(syns)\n",
    "                \n",
    "        # to form sentence with detokenizer\n",
    "        processed.append(detokenizer.detokenize(lemmatized))\n",
    "        \n",
    "    return processed\n",
    "    \n",
    "processed_train = data_preprocessing(train_texts)\n",
    "processed_test = data_preprocessing(test_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IUJunnfXItQ"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 2: Feature Engineering (I) - TF-IDF as features (10 Marks)\n",
    "\n",
    "In the lectures we have seen that raw counts of words and `tf-idf` scores can be useful features for a classification task. Complete the following code cell to create a suggestion detector which uses `tf-idf` scores as features for a Naïve Bayes classifier.\n",
    "\n",
    "After applying your preprocessing steps, use the training data to train the classifier and make predictions on the test set. You **must not** use the test set for training.\n",
    "\n",
    "If everything is implemented correctly, then you should see a single floating point value between 0 and 1 at the end which denotes the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3gDsfB8xTGMg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Execution Time ---\n",
      "--- 0.43 minutes ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7352941176470589"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Calculate tf-idf scores for the words in the training set.\n",
    "# ... your code goes here\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "## for raw count and use character analyzer to creates n-grams that span across words and increase the robustness with regards to misspellings and word derivations\n",
    "count_vect = CountVectorizer(analyzer='char', ngram_range=(5, 5)) #analyzer='char', ngram_range=(5, 5)\n",
    "X_train_counts = count_vect.fit_transform(processed_train) \n",
    "\n",
    "## to transform raw counts to tf-idf scores --> same as using TfidfVectorizer directly.\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(processed_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "\n",
    "# Train a Naïve Bayes classifier using the tf-idf scores for words as features.\n",
    "# ... your code goes here\n",
    "NB_classifier_tfidf = GaussianNB()\n",
    "NB_classifier_tfidf.fit(X_train_tfidf.toarray(), train_labels)\n",
    "\n",
    "\n",
    "# Predict on the test set.\n",
    "predictions = []    # save your predictions on the test set into this list\n",
    "\n",
    "# ... your code goes here\n",
    "predictions = NB_classifier_tfidf.predict(X_test_tfidf.toarray())\n",
    "print(\"--- Execution Time ---\")\n",
    "print(\"--- %.2f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "\n",
    "#################### DO NOT EDIT BELOW THIS LINE #################\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "  '''\n",
    "  Calculate the accuracy score for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  assert len(labels) == len(predictions)\n",
    "  \n",
    "  correct = 0\n",
    "  for label, prediction in zip(labels, predictions):\n",
    "    if label == prediction:\n",
    "      correct += 1 \n",
    "  \n",
    "  score = correct / len(labels)\n",
    "  return score\n",
    "\n",
    "# Calculate accuracy score for the classifier using tf-idf features.\n",
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDx_M2aTIncl"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 3: Evaluation Metrics (10 marks)\n",
    "\n",
    "Why is accuracy not the best measure for evaluating a classifier? Describe an evaluation metric which might work better than accuracy for a classification task such as suggestion detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8jDzSU86xI1"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 150 words.\n",
    "\n",
    "---\n",
    "\n",
    "Accuracy is not suitable for measuring model performance on an imbalanced classification issue as it could not represent the numbers of correctly classified samples of different classes. We could simply predict only the majority class to get a high accuracy in an imbalanced dataset. Since in real world, most of the classification tasks including suggestion detection has no balanced dataset, hence it is not ideal to measure with accuracy. \n",
    "\n",
    "Other than accuracy, we have better options to measure the performance of model including precision and recall. Precision is to calculate the number of relevant instances in the results set (focus on false positives) while recall is to calculate the number of relevant instances being predicted (focus on false negatives). Rather than picking one over another, we could combine both the metrics into a single score to capture both properties. The combined metric is called **F-score**. Such measure helps to overcome the issue of not able to tell the whole story with a single precision or recall. \n",
    "\n",
    "We are implementing F-score in the next section to evaluate the model performance.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ozD4SyyRDL3"
   },
   "source": [
    "In the code cell below, write an implementation of the evaluation metric you defined above. Please write your own implementation from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UkUX5K0oMhKI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8392857142857143"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(labels, predictions):\n",
    "  '''\n",
    "  Calculate an evaluation score other than accuracy for a given set of predictions and labels.\n",
    "  \n",
    "  Args:\n",
    "    labels (list): A list containing gold standard labels annotated as `0` and `1`.\n",
    "    predictions (list): A list containing predictions annotated as `0` and `1`.\n",
    "\n",
    "  Returns:\n",
    "    float: A floating point value to score the predictions against the labels.\n",
    "  '''\n",
    "\n",
    "  # check that labels and predictions are of same length\n",
    "  assert len(labels) == len(predictions)\n",
    "\n",
    "  score = 0.0\n",
    "  \n",
    "  #################### EDIT BELOW THIS LINE #########################\n",
    "\n",
    "  # your code goes here\n",
    "\n",
    "  # compute confusion matrix\n",
    "  classes = len(np.unique(labels)) # Number of classes \n",
    "  cm = np.zeros((classes, classes))\n",
    "\n",
    "  for i in range(len(labels)):\n",
    "      cm[labels[i]][predictions[i]] += 1\n",
    "  \n",
    "  # f-score = 2* (precision*recall / (precision + recall)) = tp / (tp + (1/2*(fp+fn)))\n",
    "  score = cm[0][0] / (cm[0][0] + (1/2*(cm[1][0]+cm[0][1])))\n",
    "\n",
    "  #################### EDIT ABOVE THIS LINE #########################\n",
    "\n",
    "  return score\n",
    "\n",
    "# Calculate evaluation score based on the metric of your choice\n",
    "# for the classifier trained in Task 2 using tf-idf features.\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22OelF89a27J"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 4: Feature Engineering (II) - Other features (10 Marks)\n",
    "\n",
    "Describe features other than those defined in Task 2 which might improve the performance of your suggestion detector. If these features require any additional pre-processing steps, then define those steps as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EBS0F877UyC"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "In task 2, we have used `bag-of-words` to provide raw count of words and specified a character analyzer to cope with the potential misspellings or word derivations, along with `tf-idf` scores as feature extractor to re-weight the count features that leans to relevance of terms regarding the context. However, there are some challenges when using tf-idf. Although local positioning information can be kept by extracting n-grams and calculating the tf-idf scores but they discarded the inner structure of sentence and the true meaning of the context. Besides, it's not a friendly method when dealing with large vocabulary as the vast amount of vocabularies are costing high memory and computational time.\n",
    "\n",
    "To deal with the cost of memory introduced in task 2, we could implement a `hashing vectorizer`. This vectorizer is stateless which holds only constructor parameters hence there is no need to call the \"fit\" and allocate memory for  storing a vocabulary dictionary. Similar as bag-of-words, we could extract n-grams to deal with derived or misspelled words. However, hashing vectorizer could have collisions where distinct tokens can be mapped to the same feature index. Such issue could be mitigated by setting the *n_features*. This feature is ideal to deal with large corpus.\n",
    "\n",
    "Other than that, we could implement a word embedding system to identify the semantics and contextual information of the sentences which wasn’t included in raw counts and tf-idf. In addition, it could minimize the sparsity of vector by using real-value vectors of specific feature dimensions instead of 0s. It is also easier to compute the similarity between words in vector space. In this task, we are implementing `GloVe`, `FastText`, `Word2Vec` for word embeddings. Both are using pretrained model and vertical stacking along with mean pool are performed to convert the vectors as model input. Gensim downloaded is used to retrieved the pretrained models. \n",
    "\n",
    "GloVe, a hybrid unsupervised learning algorithm to generate features for tokens. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the linear structures of the word is captured similar as Word2Vec. The \"glove-wiki-gigaword-300\" pretrained model, which is based on 2B tweets, 27B tokens, 1.2M vocab, uncased, is used for GloVe. \n",
    "\n",
    "We are using the \"fasttext-wiki-news-subwords-300\" model, which pretrained on 1 million word vectors trained on Wikipedia 2017 for FastText. \n",
    "\n",
    "For Word2Vec, the \"word2vec-google-news-300\", which trained on a part of the Google News dataset (about 100 billion words). The model contains 3 million words and phrases.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfkzM3DRce14"
   },
   "source": [
    "In the code cell below, write an implementation of the features (and any additional pre-preprocessing steps) you defined above. You are free to use a library such as `nltk` or `sklearn` for this task.\n",
    "\n",
    "After creating your features, use the training data to train a Naïve Bayes classifier and use the test set to evaluate its performance using the metric defined in Task 3. You **must not** use the test set for training.\n",
    "\n",
    "To make sure that your code doesn't take too long to run or use too much memory, you can consider a time limit of 3 minutes and a memory limit of 12GB for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Execution Time ---\n",
      "--- 0.14 minutes ---\n",
      "f-score:  0.8329621380846325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "start_time = time.time()\n",
    "\n",
    "hv = HashingVectorizer(n_features=2**16, analyzer='char', ngram_range=(5, 5))\n",
    "X_train_hv = hv.transform(processed_train)\n",
    "X_test_hv = hv.transform(processed_test)\n",
    "\n",
    "NB_classifier_hash = GaussianNB()\n",
    "NB_classifier_hash.fit(X_train_hv.toarray(), train_labels)\n",
    "\n",
    "print(\"--- Execution Time ---\")\n",
    "print(\"--- %.2f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "predictions = NB_classifier_hash.predict(X_test_hv.toarray())\n",
    "print(\"f-score: \", evaluate(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HashingVectorizer uses shorter time than CountVectorizer while achieving a similar f-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(KeyedVectors, texts):\n",
    "    # use glove / word2vec / fasttext\n",
    "    all_text_vecs = []\n",
    "    oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "    tokens = []\n",
    "    # tokenize sentences again\n",
    "    for text in texts:\n",
    "        tokens.append(tokenizer.tokenize(text))\n",
    "        \n",
    "    for toks in tokens:\n",
    "        text_vecs = []\n",
    "\n",
    "        for tok in toks:\n",
    "            if tok in KeyedVectors:\n",
    "                text_vecs.append(KeyedVectors[tok])\n",
    "            else:\n",
    "                text_vecs.append(oov)\n",
    "\n",
    "        all_text_vecs.append(text_vecs)\n",
    "\n",
    "    all_pooled_vecs = []\n",
    "\n",
    "    for text_vecs in all_text_vecs:\n",
    "        # Vstack and take the mean of the tex_vecs\n",
    "        mean_pool = np.mean(np.vstack(text_vecs), axis=0)\n",
    "        all_pooled_vecs.append(mean_pool)\n",
    "\n",
    "    # Vstack to reshape\n",
    "    embedded = np.vstack(all_pooled_vecs)\n",
    "        \n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u9mRku0va8kK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (5.0.0)/charset_normalizer (2.0.11) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings. Please note this may take a few minutes.\n",
      "Finished downloading GloVe\n",
      "--- Execution Time ---\n",
      "--- 0.61 minutes ---\n",
      "f-score:  0.550326797385621\n"
     ]
    }
   ],
   "source": [
    "# Create your features.\n",
    "# ... your code goes here\n",
    "start_time = time.time()\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Downloading GloVe embeddings. Please note this may take a few minutes.\")\n",
    "glove = api.load('glove-wiki-gigaword-300')\n",
    "print(\"Finished downloading GloVe\")\n",
    "\n",
    "X_train = word_embedding(glove, processed_train)\n",
    "X_test = word_embedding(glove, processed_test)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "NB_classifier_glove = GaussianNB()\n",
    "NB_classifier_glove.fit(X_train, train_labels)\n",
    "print(\"--- Execution Time ---\")\n",
    "print(\"--- %.2f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "predictions = NB_classifier_glove.predict(X_test)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "print(\"f-score: \", evaluate(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading FastText embeddings. Please note this may take a few minutes.\n",
      "Finished downloading FastText\n",
      "--- Execution Time ---\n",
      "--- 1.47 minutes ---\n",
      "f-score:  0.3953318745441284\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(\"Downloading FastText embeddings. Please note this may take a few minutes.\")\n",
    "fasttext = api.load('fasttext-wiki-news-subwords-300')\n",
    "print(\"Finished downloading FastText\")\n",
    "\n",
    "X_train = word_embedding(fasttext, processed_train)\n",
    "X_test = word_embedding(fasttext, processed_test)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "NB_classifier_fasttext = GaussianNB()\n",
    "NB_classifier_fasttext.fit(X_train, train_labels)\n",
    "print(\"--- Execution Time ---\")\n",
    "print(\"--- %.2f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "predictions = NB_classifier_fasttext.predict(X_test)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "print(\"f-score: \", evaluate(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Word2Vec embeddings. Please note this may take a few minutes.\n",
      "Finished downloading Word2Vec\n",
      "--- Execution Time ---\n",
      "--- 0.46 minutes ---\n",
      "f-score:  0.5486610058785107\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "print(\"Downloading Word2Vec embeddings. Please note this may take a few minutes.\")\n",
    "word2vec = api.load('word2vec-google-news-300')\n",
    "print(\"Finished downloading Word2Vec\")\n",
    "\n",
    "X_train = word_embedding(word2vec, processed_train)\n",
    "X_test = word_embedding(word2vec, processed_test)\n",
    "\n",
    "# Train a Naïve Bayes classifier using the features you defined.\n",
    "# ... your code goes here\n",
    "NB_classifier_word2vec = GaussianNB()\n",
    "NB_classifier_word2vec.fit(X_train, train_labels)\n",
    "print(\"--- Execution Time ---\")\n",
    "print(\"--- %.2f minutes ---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "predictions = NB_classifier_word2vec.predict(X_test)\n",
    "\n",
    "# Evaluate on the test set.\n",
    "# ... your code goes here\n",
    "print(\"f-score: \", evaluate(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyDD1zFQdwCf"
   },
   "source": [
    "---\n",
    "\n",
    "## Task 5: Kaggle Competition (10 marks)\n",
    "\n",
    "Head over to https://www.kaggle.com/t/1f90b74da0b7484da9647638e22d1068  \n",
    "Use above classifier to predict the label for test_unseen.csv from competition page and upload the results to the leaderboard. The current baseline score is 0.36823. Make an improvement above the baseline. Please note that the evaluation metric for the competition is the f-score.\n",
    "\n",
    "Read competition page for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9NZrBayoN4A",
    "outputId": "d2c338a4-f20f-429e-9c69-a4a7850de428"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JaC6B824Fe0H"
   },
   "outputs": [],
   "source": [
    "# Preparing submission for Kaggle\n",
    "StudentID = \"22221970_Chin\" # Please add your student id and lastname\n",
    "test_unseen = pd.read_csv(\"test_unseen.csv\", names=['id', 'text'], header=0)\n",
    "\n",
    "# preparing the test_unseen dataset\n",
    "processed_test_unseen = data_preprocessing(test_unseen['text'].to_list())\n",
    "\n",
    "model_data = {\n",
    "    'tf-idf': [NB_classifier_tfidf,  tfidf_transformer.transform(count_vect.transform(processed_test_unseen)).toarray()],\n",
    "    'hash': [NB_classifier_hash, hv.transform(processed_test_unseen).toarray()],\n",
    "    'glove': [NB_classifier_glove, word_embedding(glove, processed_test_unseen)],\n",
    "    'fasttext': [NB_classifier_glove, word_embedding(fasttext, processed_test_unseen)],\n",
    "    'word2vec': [NB_classifier_glove, word_embedding(word2vec, processed_test_unseen)]\n",
    "}\n",
    "\n",
    "# Here Id is unique identifier assigned to each test sample ranging from test_0 till test_1699\n",
    "# Expected is a list of prediction made by your classifier\n",
    "sub = {\"Id\": [f\"test_{i}\" for i in range(len(test_unseen))],\n",
    "       \"Expected\": model_data['tf-idf'][0].predict(model_data['tf-idf'][1])}\n",
    "# sub\n",
    "sub_df = pd.DataFrame(sub)\n",
    "# The code below will generate a StudentID.csv on your drive on the left hand side in the explorer\n",
    "# Please upload the file as a submission on the competition page\n",
    "# You can index your submission StudentID_Lastname_index.csv, where index is your number of submission\n",
    "index = 5\n",
    "sub_df.to_csv(f\"{StudentID}_{index}.csv\", sep=\",\", header=1, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6brptmkqY9C"
   },
   "source": [
    "Mention the approach that you have chosen briefly, and what is the mean average f-score that you have achieved? Did it improve above the chosen baseline model (0.36823)? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZQumdT-9yet"
   },
   "source": [
    "Edit this cell to write your answer below the line in no more than 500 words.\n",
    "\n",
    "---\n",
    "\n",
    "In conclusion, I have chosen the following steps for data preprocessing:\n",
    "1. strip the first and last character\n",
    "2. remove stopwords\n",
    "3. tokenizing the texts to perform WordNet synonyms enrichment.\n",
    "4. detokenizing the texts to prepare for feature extractions.\n",
    "\n",
    "The above steps could possibly retain the information in the context while removing some of the noise. The synonyms enrichment could help to emphasize the true meaning of the context. As this is a suggestion detection task, I think it should be more appropriate to get synonyms of the adjectives and verbs which might contribute more in recognizing an act of suggestion. \n",
    "\n",
    "Furthermore, I performed feature extraction with ngrams bag-of-words and tf-idf scores. It took more than half a minute to complete the training and achieved a 0.7338 accuracy and 0.8379 f-score (~0.57 minutes) on the test set. Since this is an imbalanced binary classification task, it should be more ideal to measure the model performance with f-score. \n",
    "\n",
    "In the following task, I performed feature engineering with ngrams hashing vectorizer. It's faster than the previous feature extraction method while obtaining a similar f-score of 0.83 (\\~0.15 minutes) on the test set. I have also tried to extract features with word embeddings including GloVe, FastText, and Word2Vec. However, all of them only achieved a much lower f-score compared to the vectorizers. GloVe and Word2Vec achieved a similar f-score of about 0.55 (\\~0.61 minutes) and 0.55 (\\~0.46 minutes) respectively while FastText has the worst performance among all with an f-score of 0.4 (\\~1.47 minutes). \n",
    "\n",
    "Ergo, I have chosen the ngrams hashing vectorizer as my aproach. It achieved a 0.72 f-score on the unseen test set, which improved above the chosen baseline of 0.36823. \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
