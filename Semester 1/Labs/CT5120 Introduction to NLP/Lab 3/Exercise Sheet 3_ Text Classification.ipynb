{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2AElRcqKwD4"
   },
   "source": [
    "# Exercise Sheet 3 - Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "womLlIO8E3hZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['brown', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OGl_skwUF7J2"
   },
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer, TreebankWordTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8mCh1CbBsc7"
   },
   "source": [
    "\n",
    "# 1. Preprocessing \n",
    "\n",
    "Framework for Machine learning And Feature Extraction: **sklearn**.\n",
    "\n",
    "Classes from sklearn used:\n",
    "1. [sklearn.feature_extraction.text.CountVectorize](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn-feature-extraction-text-countvectorizer)\n",
    "  It converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "2. [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "  Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "3. [class sklearn.feature_extraction.text.TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) \n",
    "  Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "4. [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html). \n",
    "\n",
    "  Build a text report showing the main classification metrics. It shows macro-average, weighted-average and per class scores for `precision`, `recall` and `f1`.\n",
    "It also displays support, which is the actual occurance of the class/label in the dataset.\n",
    "\n",
    "\n",
    "In order to feed the text to `*CountVectorizer`, it needs to exist as sentences. As shown in the following example:\n",
    "\n",
    "| text | label |\n",
    "| ---- | ----- |\n",
    "|The capital expansion programs business firms involve multi-year budgeting true country development programs|government|\n",
    "|Now Dogtown one places creeps marrow worms get old wood veneer|mystery|\n",
    "|This claim submitted District Court dismissed 126 F.Supp.235 alleged violation 7 Clayton Act also 1 2 Sherman Act|government|\n",
    "|Mrs. Meeker struck ready seek anyone's advice least Garth's|\tmystery|\n",
    "|Richmond Va.\t|government|\n",
    "\n",
    "Essentially what we need:\n",
    "\n",
    "X: Array of sentences\n",
    "\n",
    "y: Array of corresponding labels\n",
    "\n",
    "The corpus which we are using is already tokenized. It could be used as it is.\n",
    "But in real life the corpus would rarely be tokenized, so we prepare the data as sentences and labels before proceeding with the exercise.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Tokenization And Detokenisation(instead of `.join()`)\n",
    "\n",
    "The default tokenization method in NLTK involves tokenization using regular expressions as defined in the Penn Treebank (based on English text). It assumes that the text is already split into sentences.\n",
    "\n",
    "This is a very useful form of tokenization since it incorporates several rules of linguistics to split the sentence into the most optimal tokens.\n",
    "\n",
    "Detokenizer is required to put the sentence back together from a list of words, with proper punctuation form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zjUIAp_ATv7G"
   },
   "outputs": [],
   "source": [
    "detokenizer = TreebankWordDetokenizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXWS03_dJnks"
   },
   "source": [
    "#2. Dataset And Problem Statement\n",
    "\n",
    "## [Brown Corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html)\n",
    "The corpus consists of one million words of American English texts printed in 1961. The texts for the corpus were sampled from 15 different text categories to make the corpus a good standard reference.\n",
    "\n",
    "From this dataset we select two categories:\n",
    "1. government: Text from government documents\n",
    "2. mystery: Text from mystery and detective fiction\n",
    "\n",
    "And we create our own dataset by detokenizing and shuffling the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zohpGXeAGd2A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: adventure       , Dataset Size:4637\n",
      "Category: belles_lettres  , Dataset Size:7209\n",
      "Category: editorial       , Dataset Size:2997\n",
      "Category: fiction         , Dataset Size:4249\n",
      "Category: government      , Dataset Size:3032\n",
      "Category: hobbies         , Dataset Size:4193\n",
      "Category: humor           , Dataset Size:1053\n",
      "Category: learned         , Dataset Size:7734\n",
      "Category: lore            , Dataset Size:4881\n",
      "Category: mystery         , Dataset Size:3886\n",
      "Category: news            , Dataset Size:4623\n",
      "Category: religion        , Dataset Size:1716\n",
      "Category: reviews         , Dataset Size:1751\n",
      "Category: romance         , Dataset Size:4431\n",
      "Category: science_fiction , Dataset Size:948\n",
      "\n",
      "\n",
      "Selecting `government` and `mystery` categories from brown corpus\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>My God much want\"</td>\n",
       "      <td>mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6128</th>\n",
       "      <td>\"Let's see\"</td>\n",
       "      <td>mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>Another fact Tim's disappeared--run</td>\n",
       "      <td>mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>This broth boy Felix nothing obvious joy took ...</td>\n",
       "      <td>mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>He could consider</td>\n",
       "      <td>mystery</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label\n",
       "4970                                  My God much want\"  mystery\n",
       "6128                                        \"Let's see\"  mystery\n",
       "6125                Another fact Tim's disappeared--run  mystery\n",
       "4731  This broth boy Felix nothing obvious joy took ...  mystery\n",
       "4496                                  He could consider  mystery"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    corpus_length = len(brown.sents(categories=[category]))\n",
    "    print(f'Category: {category:<16}, Dataset Size:{corpus_length}')\n",
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "print('\\n\\nSelecting `government` and `mystery` categories from brown corpus')\n",
    "\n",
    "def filter_and_join(sent_arr, lab):\n",
    "    filtered_tokens = [token for token in sent_arr if (token not in english_stopwords and token not in punctuations)]\n",
    "    return [detokenizer.detokenize(filtered_tokens), lab]\n",
    "\n",
    "## Using the filter_and_join function on all the text inputs of government categories\n",
    "government_text = list(map(lambda x: filter_and_join(x, 'government'), brown.sents(categories=['government'])))\n",
    "\n",
    "## Using the filter_and_join function on all the text inputs of government categories\n",
    "mystery_text = list(map(lambda x: filter_and_join(x, 'mystery'), brown.sents(categories=['mystery'])))\n",
    "\n",
    "dataset = pd.DataFrame(government_text + mystery_text, columns=['text', 'label'])\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4nuaQAkP1Nr"
   },
   "source": [
    "## PROBLEM STATEMENT\n",
    "\n",
    "Use the given corpus to perform the following tasks:\n",
    "\n",
    "1. Setting Test/Train dataset: Split the dataset in the train and test dataset. (10% test, 90% training)\n",
    "\n",
    "2. Feature Extraction: Use the text to extract the features i.e. Count Vectors and TFIDF.\n",
    "\n",
    "3. Train ML model: Use the extracted Features to train `Naive Bias` models (1 with each extracted feature)\n",
    "\n",
    "4. Evaluation: calculate the precision, recall and f1 score.\n",
    "  Hint: Use classification report\n",
    "\n",
    "5. Inference: Use the given strings and the trained models to predict the class/label of the text.\n",
    "\n",
    "OPTIONAL:\n",
    "Train Any other model of your choice which could do better than the naive bias model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb2FSGbIGB96"
   },
   "source": [
    "# 3. Split Data into training and testing sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwLNGNnKU8cQ"
   },
   "source": [
    "## EXERCISE 1\n",
    "Split the dataset in the train and test dataset. The test set should be 10% of the overall dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.2-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m252.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from scikit-learn) (1.23.3)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.1-cp39-cp39-macosx_12_0_arm64.whl (29.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m29.9/29.9 MB\u001b[0m \u001b[31m214.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.1.2 scipy-1.9.1 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0__LQnrqSP_T"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data =  train_test_split(dataset, test_size = 0.1) ## YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6226, 2)\n",
      "(692, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53KFI8kYZkMN"
   },
   "source": [
    "<Details>\n",
    "<summary>HINT</summary>\n",
    "Use the function\n",
    "\n",
    "```python\n",
    "train_test_split(dataset, test_size=???)\n",
    "```\n",
    "\n",
    "</Details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3Fv_VNWUO1O"
   },
   "source": [
    "# 4. Feature Engineering using raw counts and TF-IDF\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "The vector representation of the text using counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tiknmpTXUNtp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer1 = CountVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "X2 = vectorizer1.fit_transform(corpus)\n",
    "print(X2.toarray())\n",
    "vectorizer1.get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ3lQ6HcUhh-"
   },
   "source": [
    "- In the example above, method get_feature_names() returns vocabulary of the corpus i.e. number of unique words. \n",
    "- Each document in the corpus is represented with the reference to the vocabulary\n",
    "- Example: In the document 1 i.e. **\"This is the first document.\"** can be rearranged to **[0, \"document\", \"first\", \"is\", 0, 0, \"the\", 0, \"this\"]** which in the end transformed into count vector based on the number of times the given word occurs in the document i.e. **[0 1 1 1 0 0 1 0 1]**\n",
    "\n",
    "\n",
    "\n",
    "Example below shows the vector representation of the text using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TowshEhpU4Nq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(analyzer='word', ngram_range=(1, 1))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(X2.toarray())\n",
    "vectorizer2.get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4YxJ_OkVVYH"
   },
   "source": [
    "- Similar to count vector, each index in tf-idf vector represents word in the vocabulary.\n",
    "- Each value represents the L2 normalized tf-idf of the word in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7LgvWONQ2PJ"
   },
   "source": [
    "## FEATURE EXTRACTION FOR THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w_qvwbKgYDQb"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "X_train, y_train = train_data[\"text\"], train_data[\"label\"]\n",
    "X_test, y_test = test_data[\"text\"], test_data[\"label\"]\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train) \n",
    "\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6349              The room looked normal even commonplace\n",
       "63                                          Seattle Wash.\n",
       "1601                                 Records diaries kept\n",
       "6510    Dead dead brass door nail I sometimes feel lik...\n",
       "5315                                          You--\"Hello\n",
       "                              ...                        \n",
       "6697                                He even noticed--name\n",
       "4624         That incidentally might give idea Felix like\n",
       "6507                                         What one say\n",
       "1506    If used walls roof 10 inches thick give protec...\n",
       "2434                                                     \n",
       "Name: text, Length: 692, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMQxZmpAQlKv"
   },
   "source": [
    "## EXERCISE 2\n",
    "The features for the training set have already been generated. Now, generate the features for the test set.\n",
    "\n",
    "## WARNING: \n",
    "\n",
    "Make sure that you do not change the features based on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "CvuZcr8xS1Ld"
   },
   "outputs": [],
   "source": [
    "X_test_counts = count_vect.transform(X_test) ### YOUR CODE GOES HERE\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)### YOUR CODE GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHIuake2drZ0"
   },
   "source": [
    "<Details>\n",
    "<summary>HINT</summary>\n",
    "Use the function\n",
    "\n",
    "Do NOT use the `fit_transform` function as shown above, it is likely to add more features from the testing data, which is not the purpose of the test dataset.\n",
    "\n",
    "</Details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnQA6P-2QGyx"
   },
   "source": [
    "# 5. Naive Bias Classifier\n",
    "\n",
    "Naive Bayes is a generative classification model.\n",
    "\n",
    "A generative model learns parameters by maximizing the joint probability  ùëÉ(ùëã,ùëå)  through Bayes' rule by learning  ùëÉ(ùëå)  and  ùëÉ(ùëã|ùëå)  (where  ùëã  are features and  ùëå  are labels).\n",
    "\n",
    "Prediction with Naive Bias\n",
    "\n",
    "$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg) = \\frac{P(\\text{label}) \\times P(\\frac{\\text{features}}{\\text{label}})}{P(\\text{features})}$$\n",
    "\n",
    "Assumption that all features are independant modifies the formula to:\n",
    "\n",
    "$$P\\bigg(\\frac{\\text{label}}{\\text{features}}\\bigg)= \\frac{P(\\text{label}) * P\\big(\\frac{f_1}{\\text{label}}\\big)*...  * P\\big(\\frac{f_n}{\\text{label}}\\big)}{P(\\text{features})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "g-hy8bXjYn-f"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0RtMdYXZA4Z"
   },
   "source": [
    "# 5 Training And Evaluation\n",
    "\n",
    "\n",
    "## 5.1. Navie Bias \n",
    "\n",
    "#### Training the Gaussian Naive Bayes with word counts feature vectors (CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "wJgn7KY4xVDi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  government       0.96      0.89      0.93       308\n",
      "     mystery       0.92      0.97      0.94       384\n",
      "\n",
      "    accuracy                           0.94       692\n",
      "   macro avg       0.94      0.93      0.94       692\n",
      "weighted avg       0.94      0.94      0.94       692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets train a Gaussian Naive Bayes clasifier using counts \n",
    "NB_classifier_counts = MultinomialNB()\n",
    "NB_classifier_counts.fit(X_train_counts.toarray(), y_train)\n",
    "# evaluation\n",
    "preds = NB_classifier_counts.predict(X_test_counts.toarray())\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyG3wKBvZoqh"
   },
   "source": [
    "## EXERCISE 3\n",
    "Train Gaussian Naive Bayes using TF-IDF vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "QIAAuLHx14IB"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-22 {color: black;background-color: white;}#sk-container-id-22 pre{padding: 0;}#sk-container-id-22 div.sk-toggleable {background-color: white;}#sk-container-id-22 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-22 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-22 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-22 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-22 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-22 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-22 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-22 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-22 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-22 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-22 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-22 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-22 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-22 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-22 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-22 div.sk-item {position: relative;z-index: 1;}#sk-container-id-22 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-22 div.sk-item::before, #sk-container-id-22 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-22 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-22 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-22 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-22 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-22 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-22 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-22 div.sk-label-container {text-align: center;}#sk-container-id-22 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-22 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-22\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-64\" type=\"checkbox\" checked><label for=\"sk-estimator-id-64\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_classifier_tfidf = MultinomialNB()## Your CODE GOES HERE\n",
    "## CODE FOR TRAINING GOES HERE\n",
    "NB_classifier_tfidf.fit(X_train_tfidf.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3LkYR50Twh5"
   },
   "source": [
    "## EXERCISE 4\n",
    "Evaluate the results on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "YoUES3VTZfaV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  government       0.97      0.87      0.92       308\n",
      "     mystery       0.90      0.98      0.94       384\n",
      "\n",
      "    accuracy                           0.93       692\n",
      "   macro avg       0.94      0.92      0.93       692\n",
      "weighted avg       0.93      0.93      0.93       692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## CODE FOR EVALUATION GOES HERE\n",
    "preds = NB_classifier_tfidf.predict(X_test_tfidf.toarray())\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FxStUIwKAZJ"
   },
   "source": [
    "#6. Random Examples (Tv Reviews from internet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "y6_9DxD3Zxwd"
   },
   "outputs": [],
   "source": [
    "citizen_info_ireland = '. The Government is chosen by and is collectively responsible to the D√°il. \\\n",
    "                        There must be a minimum of 7 and a maximum of 15 Ministers. \\\n",
    "                        The Taoiseach, the Tanaiste and the Minister for Finance must be members of the D√°il.\\\n",
    "                        It is possible to have 2 Ministers who are members of the Senate but this rarely happens.'\n",
    "gone_girl_review = 'Audience Reviews for Gone Girl ... \\\n",
    "                          Mesmerizing performances, tense atmosphere, unexpected plot twists and turns \\\n",
    "                          of events, this movie is a real crime thriller!'\n",
    "\n",
    "sherlock_bbc_review = 'Dr Watson, a former army doctor, finds himself sharing a flat with Sherlock Holmes, \\\n",
    "                        an eccentric individual with a knack for solving crimes. Together, they take on the most unusual cases.'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. The Government is chosen by and is collectively responsible to the D√°il.                         There must be a minimum of 7 and a maximum of 15 Ministers.                         The Taoiseach, the Tanaiste and the Minister for Finance must be members of the D√°il.                        It is possible to have 2 Ministers who are members of the Senate but this rarely happens.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citizen_info_ireland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZ9e4q32UUPP"
   },
   "source": [
    "## EXERCISE 5.1\n",
    "Predict the labels for the above text, using either of the model trained in exercise 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "JwLCPCUJbFOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB_classifier_counts:  ['government' 'mystery' 'government']\n",
      "NB_classifier_tfidf:  ['government' 'mystery' 'government']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "counts = count_vect.transform([citizen_info_ireland, gone_girl_review, sherlock_bbc_review]) ### YOUR CODE GOES HERE\n",
    "preds = NB_classifier_counts.predict(counts.toarray())\n",
    "print(\"NB_classifier_counts: \", preds)\n",
    "\n",
    "tfidfs = tfidf_transformer.transform(counts)### YOUR CODE GOES HERE\n",
    "preds = NB_classifier_tfidf.predict(tfidfs.toarray())\n",
    "print(\"NB_classifier_tfidf: \", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kODkEouYh1qd"
   },
   "source": [
    "<Details>\n",
    "<summary>HINT</summary>\n",
    "This step is somewhat like the step that is performed on the test set.\n",
    "But keep in mind that the feature extraction step is still to be performed before the prediction step.\n",
    "\n",
    "</Details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D4aMljRd-Ou"
   },
   "source": [
    "# [OPTIONAL] Exercise\n",
    "## 5.2 Train a Classifier of your choice which performs better than the previous one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "XNYIhcIebG4m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-24 {color: black;background-color: white;}#sk-container-id-24 pre{padding: 0;}#sk-container-id-24 div.sk-toggleable {background-color: white;}#sk-container-id-24 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-24 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-24 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-24 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-24 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-24 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-24 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-24 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-24 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-24 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-24 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-24 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-24 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-24 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-24 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-24 div.sk-item {position: relative;z-index: 1;}#sk-container-id-24 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-24 div.sk-item::before, #sk-container-id-24 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-24 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-24 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-24 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-24 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-24 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-24 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-24 div.sk-label-container {text-align: center;}#sk-container-id-24 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-24 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-24\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;count_vect&#x27;, CountVectorizer()),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;SGD_classifier&#x27;,\n",
       "                 SGDClassifier(max_iter=5, random_state=42, tol=None))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-68\" type=\"checkbox\" ><label for=\"sk-estimator-id-68\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;count_vect&#x27;, CountVectorizer()),\n",
       "                (&#x27;tfidf_transformer&#x27;, TfidfTransformer()),\n",
       "                (&#x27;SGD_classifier&#x27;,\n",
       "                 SGDClassifier(max_iter=5, random_state=42, tol=None))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-69\" type=\"checkbox\" ><label for=\"sk-estimator-id-69\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-70\" type=\"checkbox\" ><label for=\"sk-estimator-id-70\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-71\" type=\"checkbox\" ><label for=\"sk-estimator-id-71\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDClassifier</label><div class=\"sk-toggleable__content\"><pre>SGDClassifier(max_iter=5, random_state=42, tol=None)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('count_vect', CountVectorizer()),\n",
       "                ('tfidf_transformer', TfidfTransformer()),\n",
       "                ('SGD_classifier',\n",
       "                 SGDClassifier(max_iter=5, random_state=42, tol=None))])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## YOUR CODE GOES HERE\n",
    "# with count vector\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "SGD_classifier = Pipeline([\n",
    "     ('count_vect', CountVectorizer()),\n",
    "     ('tfidf_transformer', TfidfTransformer()),\n",
    "     ('SGD_classifier', SGDClassifier(loss='hinge', \n",
    "                                      penalty='l2',\n",
    "                                      alpha=1e-4, \n",
    "                                      random_state=42,\n",
    "                                      max_iter=5, \n",
    "                                      tol=None\n",
    "                                     )),\n",
    " ])\n",
    "\n",
    "SGD_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  government       0.95      0.96      0.96       308\n",
      "     mystery       0.97      0.96      0.96       384\n",
      "\n",
      "    accuracy                           0.96       692\n",
      "   macro avg       0.96      0.96      0.96       692\n",
      "weighted avg       0.96      0.96      0.96       692\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9609826589595376"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = SGD_classifier.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD_classifier:  ['mystery']\n"
     ]
    }
   ],
   "source": [
    "preds = SGD_classifier.predict([citizen_info_ireland])\n",
    "print(\"SGD_classifier: \", preds)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOHvrhp8s+OdOfllvYnt3me",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
