{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcpXfETcx_D5"
   },
   "source": [
    "# Learning Objectives\n",
    "- Explore vectorized word representations\n",
    "- Learn to use word representations as features for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMLhM-HO0ecz"
   },
   "source": [
    "# Setup\n",
    "Please run the cells below before starting the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g9IM3VspjjxY"
   },
   "outputs": [],
   "source": [
    "!pip install datasets >> dev.null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83ffTi7f9SOI",
    "outputId": "5b6879ba-59ec-46cb-ee54-4f1c7b134284"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zhejing/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0.tar.gz (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from gensim) (1.23.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-4.2.0-cp39-cp39-macosx_11_0_arm64.whl size=23933049 sha256=c36248461cc95af26428edf62a1b1915b2ae560feebfa51c8520cce89e97a717\n",
      "  Stored in directory: /Users/zhejing/Library/Caches/pip/wheels/ed/5e/79/d2997e72ba8900a820dd5870a3566779e52ee8279f71b4c799\n",
      "Successfully built gensim\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uIaq8-BkNiV",
    "outputId": "a56bd544-cce3-4d9c-8d0c-a20cf66fd605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings. Please note this may take a few minutes.\n",
      "[=======================---------------------------] 47.5% 178.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================----------------------] 56.1% 210.8/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
      "Finished downloading GloVe\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "print(\"Downloading GloVe embeddings. Please note this may take a few minutes.\")\n",
    "glove = api.load('glove-wiki-gigaword-300')\n",
    "print(\"Finished downloading GloVe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o35REPWd1ERx"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "In this section we will explore the concept of **embeddings**, which are vector representations of words. Sometimes this term is more strictly applied only to dense vectors like **word2vec**, rather than sparse **TF-IDF** or **PPMI** *(positive pointwise mutual information)* vectors. The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another.\n",
    "\n",
    "As you recall, computers cannot directly understand language. We need to represent language, which is discrete and symbolic, as numeric values to the machine. In previous labs we've done this by using **bag-of-words** and **TF-IDF**. A limitation to those approaches is that they don't encode any sort of meaning. \n",
    "\n",
    "Modern word-level representations use fixed dimension arrays of floating point values to represent words. These word embeddings can be generated using a variety of techniques (see below). To learn more about word embeddings check out **Chapter 6** in [\"Speech and Language Processing\"](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf ) and the following blogposts:\n",
    "\n",
    "1. https://jalammar.github.io/illustrated-word2vec/\n",
    "2. https://ruder.io/word-embeddings-1/ (+ the following parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gl2xJcCqhT_s"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "One of the most popular embedding models is **word2vec**, proposed by Mikolov et al. in 2013 in the following papers:\n",
    "\n",
    "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "\n",
    "As word embeddings are a key building block of deep learning models for NLP, word2vec is often assumed to belong to the same group. Technically however, word2vec is not a part of deep learning, as its architecture is neither deep nor uses non-linearities.\n",
    "\n",
    "Essentially, word2vec is an umbrella term for two architechtures: *Skip-Gram* and *Continuous Bag-Of-Words (CBOW)*.  \n",
    "\n",
    "- **CBOW** predicts a word by its context;\n",
    "- **Skip-gram** predicts the context of a given word.\n",
    "\n",
    "![](https://machinelearningmastery.com/wp-content/uploads/2017/08/Word2Vec-Training-Models.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciK6w5F4gpNA"
   },
   "source": [
    "## GloVe\n",
    "\n",
    "**GloVe** is very similar to word2vec, but it starts off with buildings a co-occurence matrix, which makes it a *hybrid* model. \n",
    "\n",
    "It takes globacl context into considerations instead of like word2vec, which takes only local / neighbour context.\n",
    "\n",
    "> GloVe is based on ratios of probabilities from the word-word co-occurrence matrix, combining the intuitions of count-based models while also capturing the linear structures used by methods like word2vec\n",
    "*(Pennington et al. 2014)*\n",
    "\n",
    "[Here](https://www.quora.com/How-is-GloVe-different-from-word2vec) and [here](https://becominghuman.ai/mathematical-introduction-to-glove-word-embedding-60f24154e54c) are good explanations of the differences between word2vec and GloVe in more detail. \n",
    "\n",
    "The GloVe embedding model was proposed by the Stanford NLP group in 2014. Here is the article and the website for the project.\n",
    "\n",
    "* [GloVe: Global Vectors forWord Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "* [GloVe website](https://nlp.stanford.edu/projects/glove/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sng678JCgn-L"
   },
   "source": [
    "## FastText\n",
    "\n",
    "**FastText** is an embedding model proposed by Facebook in 2017.  The major difference from word2vec and GloVe is that FastText represents every word in a corpus as a set of character N-grams. For example, if `n=3`, then a vector for the word `\"where\"` will be a sum of the vectors for character trigrams `\"<wh\", \"whe\", \"her\", \"ere\", \"re>\"` (where \"<\" and \">\" are the start and end symbols). It allows us to get vectors for OOV-words and effectively work with texts that contain typos and spelling mistakes.\n",
    "\n",
    "* [Enriching Word Vectors with Subword Information](https://aclweb.org/anthology/Q17-1010) (paper)\n",
    "* [FastText website](https://fasttext.cc/)\n",
    "* [Tutorial](https://fasttext.cc/docs/en/support.html)\n",
    "* [Multilingual model](https://fasttext.cc/docs/en/crawl-vectors.html) (157 languages)\n",
    "* [Models pre-trained on Wikipedia](https://fasttext.cc/docs/en/pretrained-vectors.html) (294 languages separately)\n",
    "* [GitHub repo](https://github.com/facebookresearch/fasttext)\n",
    "\n",
    "There is a Python library `fasttext`, but you can use `gensim` to work with pre-trained models and train your own ones as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaTk_xIvg_Cz"
   },
   "source": [
    "## BPE\n",
    "\n",
    "BPEmb is a collection of pre-trained subword embeddings in 275 languages, based on **Byte-Pair Encoding (BPE)** and trained on Wikipedia. It was developed in 2018 by Benjamin Heinzerling and Michael Strube.\n",
    "\n",
    "* [Paper №1](https://arxiv.org/pdf/1508.07909.pdf) \n",
    "* [Paper №2](https://aclweb.org/anthology/L18-1473)\n",
    "* [Website](https://nlp.h-its.org/bpemb/)\n",
    "* [GitHub repo](https://github.com/bheinzerling/bpemb) for `bpemb` Python library\n",
    "\n",
    "This model uses subword information in a different way than FastText. For example, we'd like to encode the string `aaabdaaabac`. The combination `aa` is the most frequent in it, so we can combine these two characters and consider them a character of its own. This operation is performed iteratively.\n",
    "\n",
    "![](https://bpemb.h-its.org/en/en.wiki.bpe.emb.vs3000.d200.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLqf66HEpwaP"
   },
   "source": [
    "# Exploring GloVe\n",
    "\n",
    "For the first set of exercises, we will be exploring GloVe embeddings. We have preloaded a pretrained model in `glove` variable and will be using the `gensim` library to work with it.\n",
    "\n",
    "So let's start off with what exactly a word embedding is. In the cell below we look at the GloVe representation of the word \"king\". \n",
    "\n",
    "A word representation is an array of float values. In the case of GloVe it has 300 values also reffered to as having 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CjFS_0qDkH7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "shape: (300,)\n",
      "[ 0.0033901 -0.34614    0.28144    0.48382    0.59469    0.012965\n",
      "  0.53982    0.48233    0.21463   -1.0249    -0.34788   -0.79001\n",
      " -0.15084    0.61374    0.042811   0.19323    0.25462    0.32528\n",
      "  0.05698    0.063253  -0.49439    0.47337   -0.16761    0.045594\n",
      "  0.30451   -0.35416   -0.34583   -0.20118    0.25511    0.091111\n",
      "  0.014651  -0.017541  -0.23854    0.48215   -0.9145    -0.36235\n",
      "  0.34736    0.028639  -0.027065  -0.036481  -0.067391  -0.23452\n",
      " -0.13772    0.33951    0.13415   -0.1342     0.47856   -0.1842\n",
      "  0.10705   -0.45834   -0.36085   -0.22595    0.32881   -0.13643\n",
      "  0.23128    0.34269    0.42344    0.47057    0.479      0.074639\n",
      "  0.3344     0.10714   -0.13289    0.58734    0.38616   -0.52238\n",
      " -0.22028   -0.072322   0.32269    0.44226   -0.037382   0.18324\n",
      "  0.058082   0.26938    0.36202    0.13983    0.016815  -0.34426\n",
      "  0.4827     0.2108     0.75618   -0.13092   -0.025741   0.43391\n",
      "  0.33893   -0.16438    0.26817    0.68774    0.311     -0.2509\n",
      "  0.0027749 -0.39809   -0.43399    0.049531  -0.42686   -0.094679\n",
      "  0.56925    0.28742   -0.015721  -0.059162   0.1912    -0.59814\n",
      "  0.65486   -0.31363    0.16881    0.10862    0.075316   0.34093\n",
      " -0.14706    0.8359     0.39697    0.52358   -0.0096367 -0.14406\n",
      "  0.37783   -0.596     -0.063192  -0.85297   -0.3098    -1.0587\n",
      " -1.025      0.4508    -0.73324   -1.2461    -0.028488   0.20299\n",
      "  0.00259    0.31995    0.35744    0.28533    0.228      0.50956\n",
      " -0.35942    0.32683    0.046264  -0.86896   -0.2707    -0.15454\n",
      " -0.32152    0.31121    0.44134    0.85189    0.21065   -0.13741\n",
      " -0.15359   -0.059722   0.027375   0.23724   -0.39197   -0.66065\n",
      "  0.23587    0.032384  -0.64043    0.55004    0.29597    0.14989\n",
      "  0.46079   -0.26561   -0.1607    -0.36328    1.0782     0.31375\n",
      "  0.1149     0.20248    0.032748   0.41082   -0.082536   0.36606\n",
      "  0.18771    0.75415    0.079648   0.24181   -0.60319   -0.37296\n",
      " -0.047767   0.45008   -0.21135    0.022251  -0.084325   0.18644\n",
      " -0.14682    0.56571   -0.30995    0.17423   -0.41122   -0.84772\n",
      " -0.71114    0.69895   -0.13008   -0.34195   -0.30501   -0.12646\n",
      "  0.29957   -0.43488    0.31935    0.2817    -0.20631   -0.48877\n",
      "  0.34477    0.03907    1.6198    -0.6352    -0.0037675 -0.41271\n",
      "  0.30704   -0.50486    0.036385  -0.046386  -0.12004    0.010029\n",
      " -0.49116    0.041486   0.002979  -0.57694   -0.42088   -0.063218\n",
      "  0.0034244 -0.25093   -0.39689   -0.36984    0.32689    0.01385\n",
      "  0.23634   -0.055199  -0.58453    0.13211    0.50943    0.25198\n",
      " -0.0088309 -0.21273   -0.48423    0.5234    -0.32832   -0.013821\n",
      "  0.15812    0.46696    0.036822  -0.090878   0.18854    0.20794\n",
      " -0.42682    0.59705    0.53109    0.19185   -0.16392    0.064956\n",
      " -0.36009   -0.59882   -0.28134    0.1017     0.02601    0.44298\n",
      " -0.31922   -0.22432    0.7828     0.041307   0.1742     0.27777\n",
      "  0.43792   -0.84324    0.27012   -0.21547    0.52408   -0.19426\n",
      " -0.21878   -0.20713    0.092994  -0.15804    0.28716   -0.11911\n",
      " -0.20688   -0.36482    0.68548   -0.10394   -0.49974   -0.47038\n",
      " -1.2953    -0.46236    0.44467    0.13337    0.88762   -0.26494\n",
      "  0.080676  -0.20625   -0.51232    0.31112    0.062035   0.30302\n",
      " -0.33344   -0.20924   -0.17348   -0.43434   -0.45743   -0.077803\n",
      " -0.33248   -0.078633   0.82182    0.082088  -0.68795    0.30266  ]\n"
     ]
    }
   ],
   "source": [
    "print(type(glove[\"king\"]))\n",
    "print(f\"shape: {glove['king'].shape}\")\n",
    "print(glove[\"king\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1YJejS6q32n"
   },
   "source": [
    "What makes word embeddings powerful is their relationship to each other in higher dimensional space. Words that are similiar to each other will be closer to each other. `Gensim` has a function called `most_similiar` which returns words that are most similiar to a given word in the embedding space. Let's try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UhLJwQNBn_nQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banks', 0.7039024829864502),\n",
       " ('banking', 0.6014178395271301),\n",
       " ('central', 0.5375901460647583),\n",
       " ('credit', 0.5313779711723328),\n",
       " ('bankers', 0.5164543390274048),\n",
       " ('financial', 0.49996113777160645),\n",
       " ('investment', 0.4982146620750427),\n",
       " ('lending', 0.4970785081386566),\n",
       " ('citibank', 0.4939170777797699),\n",
       " ('monetary', 0.4813266098499298)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QNHB1KCsB5l"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "Find the most similiar word to the word \"account\". Do all these words make sense? Are there any suprising words in the list? Why do think they are there?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lUUEFcchsVPB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accounts', 0.8495128154754639),\n",
       " ('personal', 0.49062103033065796),\n",
       " ('savings', 0.48705780506134033),\n",
       " ('own', 0.47240906953811646),\n",
       " ('income', 0.47038543224334717),\n",
       " ('detailed', 0.46782127022743225),\n",
       " ('instance', 0.46126681566238403),\n",
       " ('fact', 0.4611614942550659),\n",
       " ('according', 0.46037551760673523),\n",
       " ('amount', 0.4574888348579407)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "glove.most_similar(\"account\")\n",
    "\n",
    "#detailed\n",
    "#instance\n",
    "#fact\n",
    "#according\n",
    "\n",
    "#hugely depends on the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AQ1V0XXsp1y"
   },
   "source": [
    "## Vector Similarity\n",
    "\n",
    "There are several different ways to measure how similar two word embeddings are to each other. The most common way is to use **cosine similarity** which uses angular distance between the two vectors as a measure of similarity. Cosine scores range from -1 (orthagonal) to 1 (identical). In practice cosine scores of $0.7$ and higher tend to indicate some degree of similarity between the two embeddings. Let's explore this with a simple implementation of cosinse similarity.\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Nz4ZiT3mv5eQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cosine score 1 indicate that the two vectors are identical\n",
      "1.0\n",
      "Higher cosine score indicate that the two vectors are similiar\n",
      "0.6014\n",
      "Lower cosine scores indicate that the two vectors are dissimilar\n",
      "-0.0387\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity Function\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "  \"\"\" Given two vectors return the cosine similarity \"\"\"\n",
    "  # dot product of vector1 and vector2\n",
    "  numerator = np.dot(vector1, vector2) \n",
    "  # the product of the normed vectors\n",
    "  denominator = np.linalg.norm(vector1) * np.linalg.norm(vector2) \n",
    "  return round(numerator / denominator, 4)\n",
    "\n",
    "wv1 = glove[\"bank\"]\n",
    "wv2 = glove[\"banking\"]\n",
    "wv3 = glove[\"dog\"]\n",
    "\n",
    "print(\"A cosine score 1 indicate that the two vectors are identical\")\n",
    "print(cosine_similarity(wv1, wv1))\n",
    "\n",
    "print(\"Higher cosine score indicate that the two vectors are similiar\")\n",
    "print(cosine_similarity(wv1, wv2))\n",
    "\n",
    "print(\"Lower cosine scores indicate that the two vectors are dissimilar\")\n",
    "print(cosine_similarity(wv1, wv3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMfJyGc6zeHw"
   },
   "source": [
    "## Exercise 2\n",
    "Rank the following words in terms of their similarity or related-ness to the word \"dog\" based on your intuition.\n",
    "\n",
    "- boat\n",
    "- puppy\n",
    "- terrier\n",
    "- house\n",
    "- cat\n",
    "\n",
    "\n",
    "Now, in the code cell below, rank them based on the cosine similarity with the word \"dog\". Do you find the results surprising?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mj6xcKGNzxl1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0.6817,\n",
       " 'puppy': 0.5936,\n",
       " 'terrier': 0.4289,\n",
       " 'house': 0.2871,\n",
       " 'boat': 0.2361}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "dog = glove[\"dog\"]\n",
    "nouns = {}\n",
    "nouns['puppy'] = glove[\"puppy\"]\n",
    "nouns['terrier'] = glove[\"terrier\"]\n",
    "nouns['cat'] = glove[\"cat\"]\n",
    "nouns['house'] = glove[\"house\"]\n",
    "nouns['boat'] = glove[\"boat\"]\n",
    "\n",
    "cs = {}\n",
    "\n",
    "for k in nouns:\n",
    "    cs[k] = cosine_similarity(dog, nouns[k])\n",
    "\n",
    "dict(sorted(cs.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNkkpl101wot"
   },
   "source": [
    "## Analogies with Word Embeddings\n",
    "A really cool feature of word embeddings is that since these are vectors you can perform mathematical operations such as addition and substraction on them and get interesting results. The most famous example of this being:\n",
    "`king` + `woman` - `man` = `queen`. \n",
    "\n",
    "You can think of this as an analogy where in the form of: *king is to man as woman is to X*. In `gensim`, we can pass positive and negative words to the `most_similar` function (see below). The word embeddings in the positive list are added together and words embeddings in the negative list are subtracted. For example `king + woman - man` would be expressed as `most_similar(positive=['king', 'woman'], negative=['man'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "bgGo-Fyz3KIJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713277101516724),\n",
       " ('princess', 0.5432624220848083),\n",
       " ('throne', 0.5386105179786682),\n",
       " ('monarch', 0.5347574949264526),\n",
       " ('daughter', 0.4980250597000122),\n",
       " ('mother', 0.4956442713737488),\n",
       " ('elizabeth', 0.4832652509212494),\n",
       " ('kingdom', 0.47747090458869934),\n",
       " ('prince', 0.4668240249156952),\n",
       " ('wife', 0.46473270654678345)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3Jld3dW4Pal"
   },
   "source": [
    "## Exercise 3\n",
    "Try playing around with this and seeing if you can find an interesting analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "aICstER_4Yzv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('children', 0.6878494024276733),\n",
       " ('girls', 0.6588815450668335),\n",
       " ('parents', 0.6434296369552612),\n",
       " ('mothers', 0.608021080493927),\n",
       " ('teens', 0.5940178036689758),\n",
       " ('mom', 0.5893314480781555),\n",
       " ('teenagers', 0.5759006142616272),\n",
       " ('toddlers', 0.5602193474769592),\n",
       " ('child', 0.5592862963676453),\n",
       " ('babies', 0.555264413356781)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "glove.most_similar(positive=[\"kids\", \"woman\"], negative=[\"man\"])\n",
    "\n",
    "#bias here as kids to woman is children while to man is something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0rsduSL4rgG"
   },
   "source": [
    "# Using Word Embeddings as Features\n",
    "In this section we'll look at how word embeddings can be used as features for NLP tasks like classification. \n",
    "\n",
    "Let's solve the task of *intent classification*, which consists in mapping a user utterance to a set of intents. For example, *\\\"what is my banking balance\\\"* > *\\\"balance\\\"*. \n",
    "\n",
    "To use embeddings as features, we will need to do the following:\n",
    "1. Tokenize the input\n",
    "2. Extract word embeddings for each word (if available) in the input sentence from a pretrained GloVe model and mean pool them\n",
    "4. Train and evaluate a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msjbuj13b_W8"
   },
   "source": [
    "## Classification Dataset\n",
    "\n",
    "We will work with [CLINC150](https://huggingface.co/datasets/clinc_oos), a dataset for intent classification that contains out-of-scope queries that do not fall into any of the system's supported intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "VFgiBHYPkDoO"
   },
   "outputs": [],
   "source": [
    "# Grab CLINC data and convert them into data frames\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_small.json\") as url:\n",
    "    data = json.loads(url.read().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "L1OhS0eebXtT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes: train - 7500 | test - 4500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['direct_deposit', 'carry_on', 'whisper_mode', 'text', 'recipe',\n",
       "       'smart_home', 'who_do_you_work_for', 'rewards_balance',\n",
       "       'restaurant_reservation', 'travel_notification', 'update_playlist',\n",
       "       'change_volume', 'routing', 'mpg', 'bill_balance',\n",
       "       'do_you_have_pets', 'cook_time', 'what_song', 'new_card',\n",
       "       'todo_list_update', 'traffic', 'next_song', 'where_are_you_from',\n",
       "       'tire_change', 'bill_due', 'greeting', 'taxes', 'lost_luggage',\n",
       "       'change_accent', 'todo_list', 'last_maintenance', 'make_call',\n",
       "       'gas_type', 'cancel_reservation', 'schedule_meeting', 'find_phone',\n",
       "       'insurance_change', 'improve_credit_score', 'travel_suggestion',\n",
       "       'roll_dice', 'repeat', 'play_music', 'are_you_a_bot',\n",
       "       'sync_device', 'calendar', 'insurance', 'international_visa',\n",
       "       'freeze_account', 'shopping_list', 'oil_change_when',\n",
       "       'share_location', 'what_can_i_ask_you', 'plug_type', 'vaccines',\n",
       "       'payday', 'application_status', 'next_holiday', 'tell_joke',\n",
       "       'ingredient_substitution', 'calendar_update', 'how_old_are_you',\n",
       "       'directions', 'definition', 'rollover_401k', 'pto_request_status',\n",
       "       'confirm_reservation', 'expiration_date', 'calories', 'timer',\n",
       "       'transfer', 'book_flight', 'change_ai_name', 'apr',\n",
       "       'accept_reservations', 'exchange_rate', 'pay_bill', 'weather',\n",
       "       'current_location', 'cancel', 'restaurant_reviews', 'pin_change',\n",
       "       'account_blocked', 'what_are_your_hobbies', 'oil_change_how',\n",
       "       'reminder_update', 'car_rental', 'pto_balance', 'translate',\n",
       "       'user_name', 'how_busy', 'yes', 'replacement_card_duration',\n",
       "       'what_is_your_name', 'gas', 'tire_pressure', 'thank_you',\n",
       "       'pto_request', 'meal_suggestion', 'fun_fact', 'nutrition_info',\n",
       "       'card_declined', 'ingredients_list', 'distance', 'book_hotel',\n",
       "       'travel_alert', 'damaged_card', 'flip_coin',\n",
       "       'restaurant_suggestion', 'min_payment', 'balance',\n",
       "       'measurement_conversion', 'w2', 'uber', 'shopping_list_update',\n",
       "       'change_user_name', 'calculator', 'order_status', 'food_last',\n",
       "       'reset_settings', 'meeting_schedule', 'timezone', 'order_checks',\n",
       "       'spending_history', 'report_fraud', 'jump_start', 'no',\n",
       "       'who_made_you', 'interest_rate', 'report_lost_card',\n",
       "       'international_fees', 'income', 'reminder', 'change_speed',\n",
       "       'redeem_rewards', 'change_language', 'transactions',\n",
       "       'schedule_maintenance', 'date', 'pto_used', 'spelling',\n",
       "       'meaning_of_life', 'flight_status', 'credit_limit', 'maybe',\n",
       "       'alarm', 'credit_score', 'goodbye', 'time', 'order',\n",
       "       'credit_limit_change'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "train  = pd.DataFrame(data[\"train\"], columns=[\"text\", \"intent\"])\n",
    "train[\"split\"] = \"train\"\n",
    "\n",
    "test   = pd.DataFrame(data[\"test\"], columns=[\"text\", \"intent\"])\n",
    "test[\"split\"] = \"test\"\n",
    "\n",
    "print(f\"Dataset split sizes: train - {len(train)} | test - {len(test)}\")\n",
    "\n",
    "# Combine datasets into single dataframe for easier use\n",
    "dataset = pd.concat([train, test])\n",
    "\n",
    "# What labels do we have?\n",
    "dataset['intent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Bg1SQNVHc32t",
    "outputId": "6b0eee81-47cf-4b89-cfb1-c92a7d32c4d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can you walk me through setting up direct depo...</td>\n",
       "      <td>direct_deposit</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i want to switch to direct deposit</td>\n",
       "      <td>direct_deposit</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>set up direct deposit for me</td>\n",
       "      <td>direct_deposit</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how do i go about setting up direct deposit</td>\n",
       "      <td>direct_deposit</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i need to get my paycheck direct deposited to ...</td>\n",
       "      <td>direct_deposit</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          intent  split\n",
       "0  can you walk me through setting up direct depo...  direct_deposit  train\n",
       "1                 i want to switch to direct deposit  direct_deposit  train\n",
       "2                       set up direct deposit for me  direct_deposit  train\n",
       "3        how do i go about setting up direct deposit  direct_deposit  train\n",
       "4  i need to get my paycheck direct deposited to ...  direct_deposit  train"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDxfVDxK7f6F"
   },
   "source": [
    "## Exercise 4.1 \n",
    "\n",
    "Tokenize the input.  Your code should do the following:\n",
    "\n",
    "1. Tokenize all the texts in the `text` column in `dataset` dataframe (i.e. `dataset[\"text\"]`). \n",
    "2. Store this tokenized text in a new variable called `text_toks`.\n",
    "\n",
    "**Hint:** You can tokenize using the native string split method or NLTK's `word_tokenize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "o4dTvvcFkTl7",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from nltk.tokenize import word_tokenize\n",
    "# word_tokenize(dataset[\"text\"].values)\n",
    "text_toks = []\n",
    "\n",
    "for text in dataset[\"text\"].values:\n",
    "    text_toks.append(word_tokenize(text))\n",
    "    \n",
    "len(text_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb8Y43h19hnj"
   },
   "source": [
    "## Exercise 4.2\n",
    "\n",
    "Next we extract the word embeddings for each word in the input. \n",
    "- Create a list called `all_text_vecs`. \n",
    "- Loop over the list of tokenized texts. Create a secondary list called `text_vecs`. For each word in the tokenized input, look up the word embedding in GloVe and add to the `text_vecs`. After you have looked up all the words, append `text_vecs` to `all_text_vecs`. \n",
    "- Sometimes we may not find an embedding in GloVe for a word. In this case, we add a randomly generated embedding to `text_vecs` representing out-of-vocabulary input. \n",
    "\n",
    "**Hint:** You can check if a word exists in GloVe using the `in` operator. For example: ```'bank' in glove``` will return `True` but ``` 'dfdss' in glove ``` will return `False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "lF27IQFyskmv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "all_text_vecs = []\n",
    "oov = np.random.rand(1,300) # random vector to represent out-of-vocab\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for sublist in text_toks:\n",
    "    text_vecs = []\n",
    "    for tok in sublist:\n",
    "        if tok in glove:\n",
    "            text_vecs.append(glove[tok])  \n",
    "        else:\n",
    "            text_vecs.append(oov)\n",
    "\n",
    "    all_text_vecs.append(text_vecs)\n",
    "  \n",
    "assert len(all_text_vecs) == len(text_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raC0qqCLC3Z8"
   },
   "source": [
    "## Exercise 4.3\n",
    "There are different strategies for using embeddings as inputs to a model. One is to concatenate the vectors horizontally . For example, if you had 5 words in an input, where each word was represented by a 300 dimensional vector, your concatenated input would be $5 \\times 300 = 1500$ dimensional vector. \n",
    "\n",
    "Machine learning models expect fixed dimenional inputs in the form of $N \\times F$, where $N$ is the number of input rows, and $F$ is the number of features being provided to the model. The challenge in our case is that each input sentence is of variable length and we can't create fixed size matrix with that. There are ways to solve this, but it's out of scope for this exercise.\n",
    "\n",
    "Another strategy is to reduce our input features by mean pooling. What this involves is vertically stacking our embeddings and take mean down the columns. So if our input is $5 \\times 300$, we can reduce down to $1 \\times 500$. Let's take a closer look at this using the `vstack` and `mean` features of the `numpy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BauhX5hYGwrS",
    "outputId": "fd864776-96f5-445b-a182-82290adc0893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 shape (5, 300)\n",
      "Example 2 shape (2, 300)\n",
      "Shape after mean pooling Example 1:  (300,)\n",
      "Shape after mean pooling Example 2:  (300,)\n"
     ]
    }
   ],
   "source": [
    "example_input1 = [glove[\"bank\"], glove[\"account\"], glove[\"open\"], glove[\"today\"], glove[\"please\"]]\n",
    "example_input2 = [glove[\"bank\"], glove[\"account\"]]\n",
    "\n",
    "# Let stack this vertically and take a look at the shape\n",
    "example1_vstack = np.vstack(example_input1)\n",
    "example2_vstack = np.vstack(example_input2)\n",
    "print(\"Example 1 shape\", example1_vstack.shape)\n",
    "print(\"Example 2 shape\", example2_vstack.shape)\n",
    "\n",
    "# What happens if we take the average across the columns \n",
    "print(\"Shape after mean pooling Example 1: \", np.mean(example1_vstack, axis=0).shape)\n",
    "print(\"Shape after mean pooling Example 2: \", np.mean(example2_vstack, axis=0).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1M6bcynHvuv"
   },
   "source": [
    "It creates a single 300-dimensional vector that represents average of all the word embeddings. In the code below let's add in the mean pool operation for each set of word embedding inputs. We will update the `dataset` dataframe with the mean pooled embeddings.\n",
    "\n",
    "1. Loop over each list of word embeddings per input\n",
    "2. `vstack` and take the mean of the `tex_vecs`\n",
    "3. Append the mean pooled vector to `all_pooled_vecs`\n",
    "4. Update dataset with these pooled vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "jII6YFvcu3N3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: True\n",
      "Sanity check: True\n"
     ]
    }
   ],
   "source": [
    "all_pooled_vecs = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "for sublist in all_text_vecs:\n",
    "    vstack = np.vstack(sublist)\n",
    "    pooled_vecs = np.mean(vstack, axis=0)\n",
    "    all_pooled_vecs.append(pooled_vecs)\n",
    "\n",
    "# 4. Update dataset with these pooled vectors\n",
    "dataset[\"embeddings\"] = all_pooled_vecs\n",
    "\n",
    "print(f\"Sanity check: {len(all_pooled_vecs) == len(all_text_vecs)}\")\n",
    "print(f\"Sanity check: {np.vstack(all_pooled_vecs).shape[1] == 300}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qua-uDd9J1BE"
   },
   "source": [
    "## Exercise 4.4\n",
    "We have finally made it to model part. We've provided the model and evaluation code below. If step 4.1 - 4.3 were completed, the code below should run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiOpoB7xvjWO",
    "outputId": "d86dbcfe-0dc1-4362-dff3-975484539d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes X_train: (7500, 300), X_text: (4500, 300)\n",
      "Output shapes y_train: (7500,), y_text: (4500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode intent label and transform into enumerated values\n",
    "le = LabelEncoder()\n",
    "le.fit(dataset[\"intent\"])\n",
    "\n",
    "dataset[\"encoded_label\"] = le.transform(dataset[\"intent\"])\n",
    "\n",
    "# break out the encoded labels by train / test split\n",
    "y_train = dataset.query(\"split=='train'\")[\"encoded_label\"]  \n",
    "y_test  = dataset.query(\"split=='test'\")[\"encoded_label\"]\n",
    "\n",
    "X_train = np.vstack(dataset.query(\"split=='train'\")[\"embeddings\"])\n",
    "X_test = np.vstack(dataset.query(\"split=='test'\")[\"embeddings\"])\n",
    "\n",
    "print(f\"Input shapes X_train: {X_train.shape}, X_text: {X_test.shape}\")\n",
    "print(f\"Output shapes y_train: {y_train.shape}, y_text: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePE-Nsv1z5HR",
    "outputId": "74ea51cd-b803-4c8b-aa82-47569bc845bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7471111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load model and fit to training data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy metrics\n",
    "acc = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
