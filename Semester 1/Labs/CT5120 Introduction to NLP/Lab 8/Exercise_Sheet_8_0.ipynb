{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKdefrTaQgG9"
   },
   "source": [
    "# Objective\n",
    "The objective of this exercise is to observe the different representations of the same word occuring in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXkJYV0expkL"
   },
   "source": [
    "# BERT\n",
    "BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n",
    "\n",
    "Meaning that a general-purpose \"language understanding\" model is trained on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering, classification etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NGI9StTjt54Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (4.64.1)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.25.3-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.5/132.5 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (2.28.1)\n",
      "Requirement already satisfied: regex in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (2022.7.9)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp310-cp310-macosx_11_0_arm64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 9.7 MB/s eta 0:00:00\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53.tar.gz (880 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "Collecting botocore<1.29.0,>=1.28.3\n",
      "  Downloading botocore-1.28.3-py3-none-any.whl (9.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.3/9.3 MB 11.3 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Using cached s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from requests) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from requests) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from sacremoses) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from sacremoses) (1.1.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp310-cp310-macosx_11_0_arm64.whl (173 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 174.0/174.0 kB 4.9 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.8.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from transformers) (1.23.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp310-cp310-macosx_12_0_arm64.whl (3.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from botocore<1.29.0,>=1.28.3->boto3) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/Caskroom/miniforge/base/envs/pytorch_m1/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=d5f68e713c9499c9e2f6bfcb0de58092a8bc50c520a6da6d6603fd24af66ecba\n",
      "  Stored in directory: /Users/zhejing/Library/Caches/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, pyyaml, jmespath, filelock, huggingface-hub, botocore, transformers, s3transfer, boto3\n",
      "Successfully installed boto3-1.25.3 botocore-1.28.3 filelock-3.8.0 huggingface-hub-0.10.1 jmespath-1.0.1 pyyaml-6.0 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 tokenizers-0.13.1 transformers-4.23.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tqdm boto3 requests regex sentencepiece sacremoses transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xL_VpVm8v98A"
   },
   "source": [
    "# transformers\n",
    "PyTorch-Transformers is a library of state-of-the-art pre-trained models for Natural Language Processing (NLP).\n",
    "The library currently contains PyTorch implementations, pre-trained model weights, usage scripts and conversion utilities for the models like BERT, GPT, XLM, RoBERTa, BistilBERT\n",
    "\n",
    "[LINK FOR TRANSFORMERS](https://pytorch.org/hub/huggingface_pytorch-transformers/)\n",
    "\n",
    "# DistilBERT\n",
    "A smaller general-purpose language representation model which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. \n",
    "\n",
    "[MORE ABOUT DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    "\n",
    "This approach reduces the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and is 60% faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SdQ41OrouDId"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qSyPkOJLuOYk"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad34c9c964fd42bca887f7ebb15b278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f333bf1bc34953a2806288c8a3be03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768cae4c014b458786dab15ff1075de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637925a101c84b00964dfc456ba8f04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Using the tokenizer provided to us \n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sWaugpaivAwV"
   },
   "outputs": [],
   "source": [
    "def get_features_diff_context(sentence_list, word_of_interest_list):\n",
    "  \"\"\"\n",
    "  sentence_list: a list of sentence\n",
    "  word_of_interest_list: list of words which occur in the corresponding sentence, \n",
    "                          whose representation we are interested in\n",
    "  \n",
    "  return dict\n",
    "   key: word_s:{1,2,3...n } - word of interest and the index of the centre it occurs in.\n",
    "   value: \n",
    "  \"\"\"\n",
    "  assert len(sentence_list) == len(word_of_interest_list)\n",
    "  sentence_list = [sentence.lower() for sentence in sentence_list]\n",
    "  inputs = tokenizer(sentence_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "  outputs = model(**inputs)\n",
    "  reps = outputs['last_hidden_state']\n",
    "  out_dict = {}\n",
    "  # words_of_interest = ['good', 'good', 'good', 'good', 'excellent']\n",
    "  for i, tokens in enumerate(inputs['input_ids'].tolist()):\n",
    "    for tok_pos, tok_indx in enumerate(tokens):\n",
    "      tok = tokenizer.convert_ids_to_tokens(tok_indx)\n",
    "      if tok == word_of_interest_list[i]:\n",
    "        out_dict[f'{tok}_s:{i}'] = reps[i, tok_pos, :].detach().numpy()\n",
    "  return out_dict            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQxMXvG54aAT",
    "outputId": "4d2bc94c-adda-4b5b-944b-9a8ba0714467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The representations obtained from sentence_list and words_of_interest:\n",
      "key: `bank_s:0`, representation_dimensions (768,)\n",
      "key: `bank_s:1`, representation_dimensions (768,)\n"
     ]
    }
   ],
   "source": [
    "sentence_list = [\"the river bank was quite nice\", \"The bank ran out of money\"]\n",
    "word_of_interest_list = [\"bank\", \"bank\"]\n",
    "\n",
    "bank_different_representations = get_features_diff_context(sentence_list, word_of_interest_list)\n",
    "\n",
    "print('The representations obtained from sentence_list and words_of_interest:')\n",
    "for k,v in bank_different_representations.items():\n",
    "  print(f'key: `{k}`, representation_dimensions {v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1GZ18XG5v0m"
   },
   "source": [
    "# Exercise 1\n",
    "Similar to the above example:\n",
    "Get the representation of:\n",
    "\n",
    "a. `good` from the sentence: `that is quite good`\n",
    "\n",
    "b. `good` from `that is very good`\n",
    "\n",
    "c. `good` from `that can be good`\n",
    "\n",
    "d. `bad` from `that is bad`\n",
    "\n",
    "Store the result of `get_features_diff_context` in a variable named `word_feature_dict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pUBR1KjuyPPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The representations obtained from sentence_list and words_of_interest:\n",
      "key: `good_s:0`, representation_dimensions (768,)\n",
      "key: `good_s:1`, representation_dimensions (768,)\n",
      "key: `good_s:2`, representation_dimensions (768,)\n",
      "key: `bad_s:3`, representation_dimensions (768,)\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE GOES HERE\n",
    "sentence_list = [\"that is quite good\", \"that is very good\", \"that can be good\", \"that is bad\"]\n",
    "word_of_interest_list = [\"good\", \"good\", \"good\", \"bad\"]\n",
    "\n",
    "word_feature_dict = get_features_diff_context(sentence_list, word_of_interest_list)\n",
    "\n",
    "print('The representations obtained from sentence_list and words_of_interest:')\n",
    "for k,v in word_feature_dict.items():\n",
    "  print(f'key: `{k}`, representation_dimensions {v.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "x70tfsba4H0w"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'good_s:0': (768,), 'good_s:1': (768,), 'good_s:2': (768,), 'bad_s:3': (768,)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Run this cell as it is:\n",
    "{key: value.shape for key, value in word_feature_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HNtCfVs88HR"
   },
   "source": [
    "expected result:\n",
    "```bash\n",
    "{'good_s:0': (768,), 'good_s:1': (768,), 'good_s:2': (768,), 'bad_s:3': (768,)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tSEPJ6g9OhC"
   },
   "source": [
    "# Exercise 2:\n",
    "Implement a similarity function that takes the previously generated set of key and features and calculates the cosine similarity of one representation with all other representation except itself.\n",
    "\n",
    "e.g. of output:\n",
    "\n",
    "```python\n",
    "{\n",
    "  'good_s:0 & good_s:1': COSINE_SIMILRITY_VALUE,\n",
    "  'good_s:0 & good_s:2': COSINE_SIMILRITY_VALUE,\n",
    "  'good_s:0 & bad_s:3': COSINE_SIMILRITY_VALUE,\n",
    "  .\n",
    "  .\n",
    "  .\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "qYiT5n9TBaGe"
   },
   "outputs": [],
   "source": [
    "def similarity(rep_dict):\n",
    "    keys = list(rep_dict.keys())\n",
    "    out_dict = {}\n",
    "    \n",
    "    for i in range(len(rep_dict)):\n",
    "        for j in range(len(rep_dict)):\n",
    "            if i == j : continue\n",
    "          # dot product of vector1 and vector2\n",
    "            numerator = np.dot(rep_dict[keys[i]], rep_dict[keys[j]]) \n",
    "          # the product of the normed vectors\n",
    "            denominator = np.linalg.norm(rep_dict[keys[i]]) * np.linalg.norm(rep_dict[keys[j]]) \n",
    "        \n",
    "            out_dict[f'{keys[i]} & {keys[j]}'] = round(numerator / denominator, 4)\n",
    "\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Sfu2_3dSlvt3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_s:0 & good_s:1: 0.9825999736785889\n",
      "good_s:0 & good_s:2: 0.9071999788284302\n",
      "good_s:0 & bad_s:3: 0.8004999756813049\n",
      "good_s:1 & good_s:0: 0.9825999736785889\n",
      "good_s:1 & good_s:2: 0.9132000207901001\n",
      "good_s:1 & bad_s:3: 0.8098000288009644\n",
      "good_s:2 & good_s:0: 0.9071999788284302\n",
      "good_s:2 & good_s:1: 0.9132000207901001\n",
      "good_s:2 & bad_s:3: 0.8065000176429749\n",
      "bad_s:3 & good_s:0: 0.8004999756813049\n",
      "bad_s:3 & good_s:1: 0.8098000288009644\n",
      "bad_s:3 & good_s:2: 0.8065000176429749\n"
     ]
    }
   ],
   "source": [
    "similarity_dict = similarity(word_feature_dict)\n",
    "for k,v in similarity_dict.items():\n",
    "  print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsW8sb86-qno"
   },
   "source": [
    "# Exercise 3\n",
    "1. In the results of `Exercise 3` why the representation of `good` has high cosine similarity in sentence 0 and sentence 1, while similarity of both of token `good` in both the sentences 0,1  is low when compared to `good` of sentence 2.\n",
    "\n",
    "2. How the representations obtained here differ from the token representation of GLOVE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. The good in sentence 0 and 1 both has similar meaning while the meaning of good in sentence 2 is slightly different.\n",
    "hence the cosine similarity is measuring the similarity of the term in the context, instead of the term itself.\n",
    "\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
